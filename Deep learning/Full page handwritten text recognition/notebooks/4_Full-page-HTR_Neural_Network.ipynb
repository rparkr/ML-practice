{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTLj0mqPO_p0"
      },
      "source": [
        "# OCR on handwritten pages\n",
        "Train a handwriting recognition model on my handwriting to automatically transcribe my mission journals.\n",
        "\n",
        "State-of-the-art research so far: [Full Page Handwriting Recognition via Image to Sequence Extraction](https://paperswithcode.com/paper/full-page-handwriting-recognition-via-image), by Sumeet S. Singh and Sergey Karayev, 11-Mar-2021. See also: [YouTube presentation from the authors](https://youtu.be/BOIrib04fmE) (~1 hr.).\n",
        "* Uses a convolutional neural network (ResNet34) as an encoder and a Transformer network as a decoder. Trained on thousands of samples from the IAM, WikiText, and proprietary datasets. The input data was also augmented to help the model generalize.\n",
        "\n",
        "## References\n",
        "* [PyTorch implementation](https://github.com/tobiasvanderwerff/full-page-handwriting-recognition) by Tobias van der Werff\n",
        "* Training data: [Text Recognition Data Generator (`trdg`)](https://github.com/Belval/TextRecognitionDataGenerator), a Python package for generating images of random text that can be used to train handwriting recognition (or OCR) models\n",
        "* Potential training data: [IAM dataset files, official website](https://fki.tic.heia-fr.ch/databases/download-the-iam-handwriting-database)\n",
        "* Potential training data: [IAM forms dataset, on Kaggle](https://www.kaggle.com/naderabdalghani/iam-handwritten-forms-dataset)\n",
        "* Potential training data: [Images of handwritten names, Kaggle](https://www.kaggle.com/landlord/handwriting-recognition)\n",
        "\n",
        "Others\n",
        "* [Stanford's CS231n course PDF outline of handwritten text recognition](http://cs231n.stanford.edu/reports/2017/pdfs/810.pdf), including a discussion of the IAM dataset and various data augmentation methods\n",
        "* [Towards Data Science article](https://towardsdatascience.com/build-a-handwritten-text-recognition-system-using-tensorflow-2326a3487cd5) and associated [GitHub repo](https://github.com/githubharald/SimpleHTR) demonstrating a handwritten text recognition task in TensorFlow using a convolutional neural network combined with a LSTM net to recognize individual words (not full paragraphs or pages)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53Oshc_G7R7S"
      },
      "source": [
        "## Research notes\n",
        "[**Research paper**](https://paperswithcode.com/paper/full-page-handwriting-recognition-via-image)\n",
        "* First successful implementation of full-page handwritten text recognition\n",
        "* Does not require uniformly formatted data (like the IAM dataset)\n",
        "* Achieves a character-error rate (CER) of about 6%\n",
        "\n",
        "[**YouTube presentation from the authors**](https://youtu.be/BOIrib04fmE)\n",
        "* Research goals that this paper addressed:\n",
        " - Multi-paragraph text detection in proper sequence\n",
        " - Capture indentation\n",
        " - Ignore scratched-out text, math, tables, or unrecognized symbols\n",
        " - Use character-level generation to avoid the constraint of a language model (with a fixed vocabulary size)\n",
        "* Prior to performing image-to-sequence extraction, you need to convert the image from grayscale to black-and-white (to assist in focusing on the text)\n",
        "* This method is called \"offline handwritten text recognition (HTR)\", which means the input is an image of text. In contrast, \"online HTR\" means that the input is pen strokes, with directional and timing data, such as what could be obtained from a digital stylus on a Microsoft Surface or Apple iPad.\n",
        "* You can include special sequence tags like `<deleted text>` or `<side note>` and the model can learn to recognize those regions\n",
        "* \"If you added a Transformer at the end of the CNN encoder, it would probably increase accuracy, but we found it wasn't necessary and model size was too large with a Transformer encoder as well\"\n",
        "* \"You don't need line-number encodings in the decoder, if you omit them, your model will perform almost as well.\"\n",
        "* \"Decoder uses local attention, though Transformers commonly use global attention. We found that local attention didn't reduce efficacy and trained faster, since we could use larger batch sizes.\"\n",
        "* \"We didn't use beam search\"\n",
        "* \"We used ResNet34 for the encoder, which has 21M parameters. You could use any CNN, though. We did not use the pre-trained version and trained ours from scratch.\"\n",
        "* \"Inference takes 4.6 seconds per page on a CPU thread, when the input image is 2500x2200 pixels with 456 characters and 11.65 lines.\"\n",
        "* \"Decoder is a Transformer with 6M parameters. We use cross-entropy loss, dropout, and cross attention on the entire image.\"\n",
        "* \"The longest part, where we spent the most time, was preparing the data to use for model training. Ideally, you want 100k samples. We had about 20k after augmenting our proprietary 13k samples. We also used WikiText rendered in over 300 fonts, with varying degrees of skewness, text layout on page, contrast, brightness, and blank images. You really need a lot of data to train these models on. The goal is to have a large enough sample base that it is independent and identically distributed, so mini-batch samples all reflect roughly the same (identical) data distribution.\"\n",
        "* \"Model doesn't perform well on out-of-distribution data.\"\n",
        "* \"The model is sensitive to image padding\"\n",
        "* Encoder takes up a lot of parameters, potentially an improvement area would be creating a more efficient encoder.\n",
        "* \"Character error rate on full-page data is about 6.3%\"\n",
        "* \"During training, weights are 32 bits, gradients are 16 bits\"\n",
        "* \"We use AWS Lambda for inferencing, so inferencing happens on CPUs\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xf2s7YM7R72"
      },
      "source": [
        "# Neural Network\n",
        "Adapted from [Full Page Handwriting Recognition via Image to Sequence Extraction](https://paperswithcode.com/paper/full-page-handwriting-recognition-via-image), by Singh et al. in March 2021. See pages 4-8 of the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK7Bl5aw7R73"
      },
      "source": [
        "## Model architecture\n",
        "\n",
        "Screenshot from pg. 4 of the paper by Singh et al.\n",
        "\n",
        "<img src=\"/imgs/Model-Architecture_FPHR_Singh-et-al_2021.png\" width=\"600\">\n",
        "\n",
        "\n",
        "By the way, Visual Studio Code has trouble loading images in Markdown if the images are stored locally. Here are other potential ways to view the image using Markdown, from [StackOverflow answer](https://stackoverflow.com/questions/32370281/how-to-embed-image-or-picture-in-jupyter-notebook-either-from-a-local-machine-o):\n",
        "\n",
        "```md\n",
        "![Screenshot from pg. 4 of the paper by Singh et al.](https://drive.google.com/uc?id=1HQL3j2eVXS_N5KEHDpvWR4YVE1jhEcg3&export=download)\n",
        "\n",
        "# Original link: https://drive.google.com/file/d/1HQL3j2eVXS_N5KEHDpvWR4YVE1jhEcg3/view\n",
        "\n",
        "<img src=\"Model-Architecture_FPHR_Singh-et-al_2021.png\" style=\"height:300px\">\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONmMlcLj7R73"
      },
      "source": [
        "## Library imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uOXKGJv8N7L",
        "outputId": "19a64b3b-6ab5-4c06-d146-2f1a1607b0ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive for access to data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set variables with paths to the image and label folders, which are accessed by\n",
        "# the dataset on initialization. The expected folder structure looks like this:\n",
        "\n",
        "# /content/drive/MyDrive/data\n",
        "#  |-- training_data\n",
        "#    |-- labels\n",
        "#      |-- train\n",
        "#      |-- test\n",
        "#    |-- processed_images\n",
        "#      |-- train\n",
        "#      |-- test\n",
        "#  |-- journal_data\n",
        "#    |-- labels\n",
        "#      |-- train\n",
        "#      |-- test\n",
        "#    |-- processed_images\n",
        "#      |-- train\n",
        "#      |-- test\n",
        "\n",
        "# Where training_data has generated images of full-page text\n",
        "# and journal_data has images of actual handwritten text\n",
        "\n",
        "path_to_image_folder = r'/content/drive/MyDrive/School/Deep learning final project/training_data/processed_images'\n",
        "path_to_label_folder = r'/content/drive/MyDrive/School/Deep learning final project/training_data/labels'\n",
        "\n",
        "# File paths to the folders with label data\n",
        "label_folders = [\n",
        "    r'/content/drive/MyDrive/School/Deep learning final project/training_data/labels/train',\n",
        "    r'/content/drive/MyDrive/School/Deep learning final project/training_data/labels/test',\n",
        "    r'/content/drive/MyDrive/School/Deep learning final project/journal_data/labels/train',\n",
        "    r'/content/drive/MyDrive/School/Deep learning final project/journal_data/labels/test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2NIHn20BPM2",
        "outputId": "0e8d70ea-837f-4438-966e-afc1e548b350"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.11.0+cu113)\n",
            "Requirement already satisfied: pyDeprecate==0.3.* in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (0.3.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.8)\n"
          ]
        }
      ],
      "source": [
        "%pip install torchmetrics\n",
        "from torchmetrics import Metric             # Package for computing measurements on PyTorch models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I1s1EYz7R73",
        "outputId": "f4091e7d-64f4-48b1-bd1a-3e2d4a0608b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU is available, device set to GPU.\n"
          ]
        }
      ],
      "source": [
        "# PyTorch modules for neural network\n",
        "import torch                                # PyTorch\n",
        "import torch.nn as nn                       # Neural network module\n",
        "import torch.optim as optim                 # Optimizer module (e.g., SGD, Adam)\n",
        "import torchvision                          # Computer vision module\n",
        "from torchvision import transforms          # Image augmentation\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"GPU is available, device set to GPU.\" if torch.cuda.is_available() \n",
        "        else \"GPU unavailable, device set to CPU.\")\n",
        "\n",
        "# Other packages\n",
        "import numpy as np                          # Array and mathematical functions\n",
        "from tqdm.notebook import tqdm              # Progress bars\n",
        "import matplotlib.pyplot as plt             # Plotting functionality\n",
        "from PIL import Image                       # Loading and manipulating images\n",
        "import random                               # Create train-test split of data\n",
        "import shutil                               # Move files\n",
        "import os                                   # Work with files and folders\n",
        "import math                                 # Use the log() function (for positional encoding)\n",
        "import string                               # Character lists (e.g., for character-level vocab)\n",
        "import editdistance                         # Implementation of Levenshtein edit distance\n",
        "import gc                                   # Garbage collector, for clearing memory\n",
        "from typing import Callable, Optional       # Modules for type annotations (type hints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnUzgeYwBQSN",
        "outputId": "e5670eb5-cde5-43bc-d325-e236b0f6d26a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue May  3 16:42:02 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8    11W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll_g6h-07R74"
      },
      "source": [
        "## Metrics and utility functions\n",
        "Adapted from: Tobias van der Werff's GitHub repo: [full-page-handwriting-recognition -> metrics.py](https://github.com/tobiasvanderwerff/full-page-handwriting-recognition/blob/master/src/metrics.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB0G0lYc7R76"
      },
      "outputs": [],
      "source": [
        "# from torchmetrics import Metric\n",
        "# import editdistance\n",
        "\n",
        "class CharacterErrorRate(Metric):\n",
        "    '''\n",
        "    Calculates the character error rate, a measurement of the percentage\n",
        "    of characters that were predicted incorrectly.\n",
        "\n",
        "    Calculation: Levenshtein edit distance / length of target.\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.add_state(\"edits\", default=torch.Tensor([0]), dist_reduce_fx=\"sum\")\n",
        "        self.add_state(\"total_chars\", default=torch.Tensor([0]), dist_reduce_fx=\"sum\")\n",
        "    \n",
        "    def update(self, predictions, targets):\n",
        "        '''\n",
        "        Updates the running count of the number of edits\n",
        "        and ground truth characters.\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        `predictions`: Tensor of shape (batch_size, predicted_characters)\n",
        "        `targets`: Tensor of shape (batch_size, target_characters)\n",
        "        '''\n",
        "        assert predictions.ndim == targets.ndim\n",
        "        \n",
        "        sos_token_idx = token_to_index['<START>']\n",
        "        eos_token_idx = token_to_index['<END>']\n",
        "\n",
        "        # Check if the first token is the start token (i.e., the standard situation)\n",
        "        if (predictions[:, 0] == sos_token_idx).all():\n",
        "            # Remove the start token\n",
        "            predictions = predictions[:, 1:]\n",
        "        \n",
        "        eos_idxs_pred = (predictions == eos_token_idx).float().argmax(dim=1).tolist()\n",
        "        eos_idxs_tgt = (targets == eos_token_idx).float().argmax(dim=1).tolist()\n",
        "\n",
        "        for i, (p, t) in enumerate(zip(predictions, targets)):\n",
        "            eos_idx_p, eos_idx_t = eos_idxs_pred[i], eos_idxs_tgt[i]\n",
        "            p = p[:eos_idx_p] if eos_idx_p else p\n",
        "            t = t[:eos_idx_t] if eos_idx_t else t\n",
        "            # Convert the predictions and target tensors to\n",
        "            # strings of indexes.\n",
        "            # Note that the editdistance package can handle\n",
        "            # any hashable iterable (like lists of strings), not just strings.\n",
        "            # see: https://pypi.org/project/editdistance/#distance-with-any-object\n",
        "            p_str, t_str = map(lambda tsr: str(tsr.flatten().tolist()), (p, t))\n",
        "            edit_d = editdistance.eval(p_str, t_str)\n",
        "\n",
        "            self.edits += edit_d\n",
        "            self.total_chars += t.numel()\n",
        "        \n",
        "    def compute(self):\n",
        "        '''Calculate the character error rate'''\n",
        "        return self.edits.float() / self.total_chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "s8GH4YND7R77",
        "outputId": "1276fe86-2f2a-4e9e-e608-c48b142a0dac"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'9831491'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Testing the tensor-to-string function used on line 49 in the cell above\n",
        "t = torch.tensor([[9, 8, 3], [1, 4, 91]])\n",
        "to_str = \"\".join(map(str, t.flatten().tolist()))\n",
        "to_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74,
          "referenced_widgets": [
            "514b204d031c4b64a5abea72e0477ec5",
            "e28b8faedc854ed79d9009ba2daa0c76",
            "871780eaabb94bcf961cd1b36c897488",
            "b71150e4618c439880834b3880afe0c7",
            "0346b1dc44354c10b641014bb21dcedb",
            "f215b8f5e4744000b3c11354e51312bc",
            "87f1f74416e24df4bde1927114be8bd0",
            "baa7ec24aeda46738bfbb68db0897761",
            "767940fcc9324e26be496e90d6a5d880",
            "6abed2ad3995436aba7ac4ec32fa88e6",
            "6c66a28dff494241bca0cd458d0fe31e"
          ]
        },
        "id": "o-KwiW4-7R78",
        "outputId": "d69403ec-b797-4e2a-eaa6-6b1010524cca"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "514b204d031c4b64a5abea72e0477ec5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of characters: 116\n",
            "dict_keys(['<START>', '<END>', '<PAD>', '<INSERT>', '</INSERT>', '\\t', '\\n', '\\x0b', '\\x0c', '\\r', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¬°', '¬ø', '√â', '√°', '√≠', '√±', '√≥', '√∫', '‚Ä¢', '‚ù§', 'üôÇ'])\n"
          ]
        }
      ],
      "source": [
        "# import string\n",
        "\n",
        "# Define a set to store character-level vocabulary from the dataset\n",
        "dataset_vocab = set()\n",
        "\n",
        "# Find all possible characters the model will encounter during training\n",
        "\n",
        "total_count = 0\n",
        "for folder in folders:\n",
        "    total_count += len(os.listdir(folder))\n",
        "\n",
        "p_bar = tqdm(total=total_count, leave=False)\n",
        "p_bar.set_description('Reading files in dataset')\n",
        "\n",
        "# Only the final two folders are needed, since those contain\n",
        "# custom-labeled data rather than generated labels. The generated\n",
        "# labels use only ascii-printable characters, but the custom labels\n",
        "# may have other characters.\n",
        "for folder in label_folders[2:4]:\n",
        "    for onefile in os.scandir(folder):\n",
        "        with open(onefile.path, mode='rt', encoding='utf-8') as labelfile:\n",
        "            # Wrapping labelfile.read() in list() converts the string to\n",
        "            # a character-level list\n",
        "            dataset_vocab.update(list(labelfile.read()))\n",
        "        p_bar.update(n=1)\n",
        "p_bar.close()\n",
        "\n",
        "\n",
        "new_characters = [char for char in dataset_vocab if char not in list(string.printable)]\n",
        "special_tokens = ['<START>', '<END>', '<PAD>', '<INSERT>', '</INSERT>']\n",
        "all_chars = list(string.printable) + new_characters\n",
        "all_chars = sorted(all_chars)\n",
        "all_chars = special_tokens + all_chars\n",
        "\n",
        "index_to_token = dict(enumerate(all_chars))\n",
        "token_to_index = {char: i for i, char in index_to_token.items()}\n",
        "\n",
        "print(f\"Total number of characters: {len(token_to_index)}\")\n",
        "print(token_to_index.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx4ossJw7R78"
      },
      "source": [
        "Text-to-tensor conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxdC5DeW7R78"
      },
      "outputs": [],
      "source": [
        "def text_to_tensor(\n",
        "    text: str,\n",
        "    token_to_idx: dict,\n",
        "    start_token: str = '<START>',\n",
        "    end_token: str = '<END>',\n",
        "    pad_token: str = '<PAD>',\n",
        "    max_len: int = 3500,\n",
        "    multichar_tokens: list = ['<INSERT>', '</INSERT>'],\n",
        "    placeholder_chars: list = ['üî¥', 'üü†'],\n",
        "    device: torch.device = device,\n",
        "    dtype: torch.dtype = torch.long) -> torch.Tensor:\n",
        "    '''\n",
        "    Takes an input string (labels) and converts it to\n",
        "    a Tensor of d_type Long (i.e., an integer Tensor),\n",
        "    where each item in the Tensor is an index into the\n",
        "    `token_to_idx` vocabulary dictionary.\n",
        "\n",
        "    Also adds the `start_token` to the beginning of\n",
        "    the tensor, the `end_token` after all the tokens\n",
        "    in the `text`, and fills the remaining space up\n",
        "    to `max_len` tokens with the `pad_token`.\n",
        "\n",
        "    Returns a Tensor of d_type Long.\n",
        "\n",
        "    Parameters\n",
        "    ---\n",
        "    `text`: str\n",
        "        The text string to be converted into a Tensor.\n",
        "        This will be the label (target) text.\n",
        "    `token_to_idx`: dict\n",
        "        A token-to-index dictionary for each token\n",
        "        in the vocabulary.\n",
        "    `start_token`: str, default='<START>'\n",
        "        The token to be added at the beginning of each \n",
        "        `text` sequence.\n",
        "    `end_token`: str, default='<END>'\n",
        "        The token to be added after all the tokens in the\n",
        "        `text` sequence.\n",
        "    `pad_token`: str, default='<PAD>'\n",
        "        The token to use for padding the given `text` up to\n",
        "        the `max_len` number of tokens.\n",
        "    `max_len`: int, default=3500\n",
        "        The maximum length (in tokens) of the provided `text`.\n",
        "        Raises an error if `text` exceeds `max_len`.\n",
        "        If `text` has fewer tokens than `max_len`, the remaining\n",
        "        tokens will be padded with `pad_token`.\n",
        "    `multichar_tokens`: list, default=['<INSERT>', '</INSERT>']\n",
        "        Since this is a character-level vocabulary, any multi-character\n",
        "        sequences that should be treated as a single token must be replaced\n",
        "        with a single-character placeholder before iterating through the `text`\n",
        "        character-by-character, and will be converted back into the \n",
        "        multi-character token during that iteration.\n",
        "    `placeholder_chars`: list, default=['üî¥', 'üü†']\n",
        "        The single-character placeholders that will take the place of\n",
        "        multi-character tokens so the function can iterate through the\n",
        "        `text` character-by-character but still recognize the correct\n",
        "        tokens. These tokens must not exist within the `token_to_idx`.\n",
        "    `device`: a torch.device object\n",
        "        The device on which to place the returned tensor. Can be either\n",
        "        torch.device('cpu') or torch.device('cuda').\n",
        "    `dtype`: a torch datatype, default=torch.long\n",
        "        See: https://pytorch.org/docs/stable/tensor_attributes.html\n",
        "        for a list of the acceptable data types. Since I am using\n",
        "        a character-level vocabulary with fewer than 256 characters,\n",
        "        torch.uint8 suits my needs and save memory.\n",
        "    '''\n",
        "    assert len(text) <= max_len, (\n",
        "        f\"The given `text` has more tokens (characters) than the `max_len` setting allows. \"\n",
        "        f\"Please increase the size of max_len to accomodate the input text. \"\n",
        "        f\"len(text)={len(text)}, and max_len={max_len}.\"\n",
        "    )\n",
        "\n",
        "    if type(multichar_tokens) != list:\n",
        "        multichar_tokens = [multichar_tokens]\n",
        "\n",
        "    if type(placeholder_chars) != list:\n",
        "        placeholder_chars = [placeholder_chars]\n",
        "    \n",
        "    for char in placeholder_chars:\n",
        "        assert char not in list(token_to_idx.keys()), (\n",
        "            f\"The provided placeholder character {char} is in the `token_to_idx`.\"\n",
        "            + \"Please use a placeholder character not found in the `token_to_idx`\"\n",
        "        )\n",
        "    \n",
        "    # List of token indices for each character in the input text.\n",
        "    # Begin the sequence with the '<START>' token.\n",
        "    index_list = [token_to_idx[start_token]]\n",
        "\n",
        "    # Replace multi-character tokens with single-character placeholders\n",
        "    for i, token in enumerate(multichar_tokens):\n",
        "        replaced_text = text.replace(token, placeholder_chars[i])\n",
        "    \n",
        "    # Iterate through the text character-by-character and return integer indices\n",
        "    for char in replaced_text:\n",
        "        if char in placeholder_chars:\n",
        "            idx = placeholder_chars.index(char)\n",
        "            index_list.append(token_to_idx[multichar_tokens[idx]])\n",
        "        else:\n",
        "            index_list.append(token_to_idx[char])\n",
        "    \n",
        "    # End the sequence with the '<END>' token.\n",
        "    index_list.append(token_to_idx[end_token])\n",
        "\n",
        "    # Fill remaining space in sequence with '<PAD>' token up to max_len\n",
        "    if len(index_list) < max_len:\n",
        "        num_pads = max_len - len(index_list)\n",
        "        pad_list = [token_to_idx[pad_token]] * num_pads\n",
        "        index_list += pad_list\n",
        "\n",
        "\n",
        "    # Convert the index_list into a Tensor\n",
        "    return torch.tensor(index_list, dtype=dtype, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gppC5BZ_7R79"
      },
      "source": [
        "Tensor-to-text conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuN8bFhl7R79"
      },
      "outputs": [],
      "source": [
        "def tensor_to_text(\n",
        "    tensor: torch.Tensor,\n",
        "    idx_to_token: dict) -> str:\n",
        "    '''\n",
        "    Takes an input Tensor and converts it to\n",
        "    a string by returning the `idx_to_token` token\n",
        "    for each index given in the tensor.\n",
        "\n",
        "    Returns a string.\n",
        "\n",
        "    Parameters\n",
        "    ---\n",
        "    `tensor`: torch.Tensor\n",
        "        The integer-type Tensor that holds indices\n",
        "        to tokens in the idx_to_token dict.\n",
        "    `idx_to_token`: dict\n",
        "        An index-to-token dictionary for each token\n",
        "        in the vocabulary.\n",
        "    '''\n",
        "    return_string = ''\n",
        "    for item in tensor:\n",
        "        return_string += idx_to_token[item.item()]\n",
        "    return return_string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeKX9bis7R7-"
      },
      "source": [
        "Testing the conversion functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDz2Hhw17R7-",
        "outputId": "b68b2623-21c5-485c-acb9-1e7ec4120644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 0, 62, 82, 83, 93, 10, 83, 93, 10, 75, 10, 94, 79, 93, 94, 10, 90, 75,\n",
            "        92, 75], device='cuda:0')\n",
            "====================\n",
            "====================\n",
            "<START>This is a test paragraph, to see if\n",
            "the text-to-tensor and tensor-to-text functions work correctly.\n",
            "    This is a sample indented line.\n",
            "\n",
            "This is a new paragraph, with a <INSERT>superscript</INSERT> added.\n",
            "\n",
            "It's also possible to have special characters, like ~ or √°.<END><PAD><PAD><PAD><PAD><PAD> ...to max_len tokens (3500 by default)\n"
          ]
        }
      ],
      "source": [
        "test_str = \"\"\"This is a test paragraph, to see if\n",
        "the text-to-tensor and tensor-to-text functions work correctly.\n",
        "    This is a sample indented line.\n",
        "\n",
        "This is a new paragraph, with a <INSERT>superscript</INSERT> added.\n",
        "\n",
        "It's also possible to have special characters, like ~ or √°.\"\"\"\n",
        "\n",
        "test_tensor = text_to_tensor(test_str, token_to_idx=token_to_index)\n",
        "print(test_tensor[:20])\n",
        "print('='*20, '\\n', '='*20, sep='')\n",
        "print(tensor_to_text(test_tensor, idx_to_token=index_to_token)[:302] + ' ...to max_len tokens (3500 by default)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qx7KSSL7R7_"
      },
      "source": [
        "## Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbHaaPWe7R8A"
      },
      "source": [
        "### Dataset\n",
        "The dataset retrieves an item (image with its associated text label) from either a 'train' or 'test' folder and returns a (image, label) tuple, where both the image and label are `torch.Tensor`s.\n",
        "\n",
        "**Augmentations**\n",
        "\n",
        "The dataset will also implement augmentation, including:\n",
        "* padding images to match the size of the largest image in the dataset (so all images in all batches are the same size). For the 10,000 training images and 10 fine-tuning images I am using, the **largest width is 2295 pixels**, and the **largest height is 1884 pixels**.\n",
        "* scaling\n",
        "* rotation\n",
        "* brightness\n",
        "* (background color?)\n",
        "* contrast\n",
        "* perspective\n",
        "* Gaussian noise\n",
        "\n",
        "During training, images can be randomly placed anywhere within the padding dimensions of the largest image in the batch, but during testing the images should be centered (still within padding).\n",
        "\n",
        "For more information on augmentation used in the paper, see Singh et al., p. 10-11 (\"Image Augmentation\" and \"Data Sampling\" sections).\n",
        "\n",
        "**Dataset references**\n",
        "* [Examples of custom datasets](https://github.com/utkuozbulak/pytorch-custom-dataset-examples)\n",
        "* [PyTorch docs: Datasets](https://pytorch.org/vision/stable/datasets.html)\n",
        "* [PyTorch docs: DataLoaders and Datasets](https://pytorch.org/docs/stable/data.html)\n",
        "* [PyTorch docs: custom datasets (in particular, see `VisionDataset`)](https://pytorch.org/vision/stable/datasets.html#base-classes-for-custom-datasets)\n",
        "\n",
        "**Transforms references**\n",
        "* [PyTorch docs: Transforming and Augmenting Images](https://pytorch.org/vision/stable/transforms.html)\n",
        "* [PyTorch docs: illustrated examples of transforms](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html)\n",
        "* [PyTorch source code (GitHub): transforms.py](https://github.com/pytorch/vision/blob/main/torchvision/transforms/transforms.py)\n",
        "\n",
        "**Note:** `torchvision.transforms.RandomApply(transforms=[], p=0.5)` will apply _all_ transformations in the list if _any_ transformation is applied (based on the probability).\n",
        "\n",
        "See also: one-step transform: [`TrivialAugmentWide`](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#trivialaugmentwide)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EcNR9TJ7R8B"
      },
      "outputs": [],
      "source": [
        "class FPHRdataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        path_to_image_folder,\n",
        "        path_to_label_folder,\n",
        "        max_width,\n",
        "        max_height,\n",
        "        augmentation_likelihood: float = 0.5,\n",
        "        train: bool = True,\n",
        "        vocab_dict: dict=token_to_index):\n",
        "        '''\n",
        "        Args\n",
        "        ---\n",
        "        `path_to_image_folder`: str\n",
        "            Root directory where images are stored. Assumes that\n",
        "            within that directory are two sub-directories:\n",
        "            'test' and 'train', each with images.\n",
        "        `path_to_label_folder`: str\n",
        "            Root directory where labels are stored. Assumes that\n",
        "            within that directory are two sub-directories:\n",
        "            'test' and 'train', each with labels.\n",
        "        `max_width`: int\n",
        "            Defines the size for each image; that is, each image will be padded\n",
        "            so its width equals max_width (in pixels). Should be at least\n",
        "            as large as the maximum width (in pixels) in any image\n",
        "            in the entire dataset.\n",
        "        `max_height`: int\n",
        "            Defines the size for each image; that is, each image will be padded\n",
        "            so its height equals max_height (in pixels). Should be at least\n",
        "            as large as the maximum height (in pixels) in any image\n",
        "            in the entire dataset.\n",
        "        `augmentation_likelihood`: float, default=0.5\n",
        "            Sets the probability that any one of the augmentation transforms\n",
        "            will be applied to the input image.\n",
        "        `train`: bool, default=True\n",
        "            If True, uses the 'train' subdirectories in the `path_...`\n",
        "            arguments. If False, uses the 'test' subdirectories.\n",
        "        `vocab_dict`: dict\n",
        "            Dictionary where keys are tokens in the vocabulary \n",
        "            (single characters, in this case), and values are indices of those\n",
        "            tokens.\n",
        "        '''\n",
        "        # No need to inherit from the base class (torch.utils.data.Dataset)\n",
        "        # That is, we don't need: super().__init__()\n",
        "\n",
        "        self.max_width = max_width\n",
        "        self.max_height = max_height\n",
        "        self.augment_prob = augmentation_likelihood\n",
        "        self.train = train\n",
        "\n",
        "        if self.train:\n",
        "            # Store the paths to the directories for images and labels\n",
        "            self.path_to_image_folder = os.path.join(path_to_image_folder, 'train')\n",
        "            self.path_to_label_folder = os.path.join(path_to_label_folder, 'train')\n",
        "        else:\n",
        "            self.path_to_image_folder = os.path.join(path_to_image_folder, 'test')\n",
        "            self.path_to_label_folder = os.path.join(path_to_label_folder, 'test')\n",
        "        \n",
        "        # Store all image file paths in memory\n",
        "        self.image_files = [os.path.join(self.path_to_image_folder, file) for file in os.listdir(self.path_to_image_folder)]\n",
        "        # Store all label file paths in memory\n",
        "        self.label_files = [os.path.join(self.path_to_label_folder, file) for file in os.listdir(self.path_to_label_folder)]\n",
        "\n",
        "\n",
        "    def create_transformations(self, padding, t_horizontal, t_vertical, augment_prob, train=True):\n",
        "        '''\n",
        "        Compose the transformations to be applied to the input image.\n",
        "        '''\n",
        "        if train:\n",
        "            # Perform augmentations with torchvision.transforms.\n",
        "            # Order of transforms:\n",
        "            # 1. ColorJitter: adjusts brightness, contrast, and saturation\n",
        "            # 2. RandomAffine: changes scale, rotates image, and translates (moves) image\n",
        "            # 3. RandomPerspective: shifts the perspective of the image\n",
        "            # 4. GaussianBlur: appplies a Gaussian blur to the image (in place of adding Gaussian noise)\n",
        "            # 5. Pad: pad image so all images are the same size \n",
        "            # 6. ToTensor: Converts the image into a torch.Tensor\n",
        "            augmentations = transforms.Compose(\n",
        "                transforms = [\n",
        "                    transforms.RandomApply(\n",
        "                        transforms = [\n",
        "                            transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
        "                            transforms.RandomAffine(degrees=2.5, translate=(t_horizontal, t_vertical), scale=(0.9, 1.0)),\n",
        "                            transforms.RandomPerspective(distortion_scale=0.1, p=augment_prob)],\n",
        "                        p = augment_prob),\n",
        "                    transforms.RandomApply(\n",
        "                        transforms = [\n",
        "                            transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 1.0))],\n",
        "                        p = augment_prob),\n",
        "                    transforms.Pad(padding=padding),\n",
        "                    transforms.ToTensor()\n",
        "                    ])\n",
        "        else:\n",
        "            # For inference (testing), just center the image within padding space\n",
        "            augmentations = transforms.Compose([transforms.Pad(padding=padding), transforms.ToTensor()])\n",
        "        \n",
        "        return augmentations\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # =====================================\n",
        "        # Set values needed for transformations\n",
        "        #  (padding, vertical and horizontal translation)\n",
        "        input_img = Image.open(self.image_files[index])\n",
        "        width, height = input_img.size\n",
        "        pad_width, pad_height = (self.max_width - width, self.max_height - height)\n",
        "\n",
        "        if any([pad_width % 2 == 1, pad_height % 2 == 1]):\n",
        "            # padding cannot be evenly split between sides of image\n",
        "            pad_left = pad_width // 2\n",
        "            pad_right = pad_width - pad_left\n",
        "            pad_top = pad_height // 2\n",
        "            pad_bottom = pad_height - pad_top\n",
        "            padding = (pad_left, pad_top, pad_right, pad_bottom)\n",
        "        else:\n",
        "            # padding can be evenly split between sides of image\n",
        "            pad_leftright = pad_width // 2\n",
        "            pad_topbottom = pad_height // 2\n",
        "            padding = (pad_leftright, pad_topbottom)\n",
        "\n",
        "        # Determine the fraction of image width and height used for translation\n",
        "        #  such that there is still padding on all sides of the image.\n",
        "        # left/right translation\n",
        "        t_horizontal = (pad_width / 2) / self.max_width\n",
        "        # top/bottom translation\n",
        "        t_vertical = (pad_height / 2) / self.max_height\n",
        "        # =====================================\n",
        "        # =====================================\n",
        "\n",
        "        # Compose the transformations for the input\n",
        "        augmentations = self.create_transformations(\n",
        "            padding = padding,\n",
        "            t_horizontal = t_horizontal,\n",
        "            t_vertical = t_vertical,\n",
        "            augment_prob = self.augment_prob,\n",
        "            train = self.train)\n",
        "        # Apply transformations\n",
        "        img_tensor = augmentations(input_img)\n",
        "        \n",
        "        with open(self.label_files[index], mode='rt', encoding='utf-8') as labelfile:\n",
        "            target_label = labelfile.read()\n",
        "        \n",
        "        target_label_tensor = text_to_tensor(\n",
        "            text = target_label,\n",
        "            token_to_idx = vocab_dict,\n",
        "            multichar_tokens = ['<INSERT>', '</INSERT>'],\n",
        "            placeholder_chars = ['üî¥', 'üü†'],\n",
        "            device = device,\n",
        "            dtype = torch.long\n",
        "        )\n",
        "\n",
        "        # print(f\"Padding size: {padding}\")\n",
        "        # print(f\"Horizontal translation percent {t_horizontal:.2f}\")\n",
        "        # print(f\"Vertical translation percent {t_vertical:.2f}\")\n",
        "        \n",
        "        return (img_tensor.to(device), target_label_tensor.to(device))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17WZm6ZW7R8D"
      },
      "source": [
        "## Positional encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY5Sw6607R8D"
      },
      "source": [
        "### 1D positional encoding (for decoder Transformer)\n",
        "\n",
        "Implementation reference: Tobias van der Werff's GitHub repo: [full-page-handwriting-recognition/src/models](https://github.com/tobiasvanderwerff/full-page-handwriting-recognition/blob/47a2d27fc40815898474e9f74badc7d544740fee/src/models.py#L18).\n",
        "\n",
        "See also:\n",
        "* [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding), Harvard's implementation of the \"Attention Is All You Need\" (2017) paper by Vaswani et al.\n",
        "* [How to Code the Transformer in PyTorch](https://blog.floydhub.com/the-transformer-in-pytorch/#giving-our-words-context-the-positional-encoding), FloydHub blog\n",
        "* [Transformers from Scratch](http://peterbloem.nl/blog/transformers#input-using-the-positions), Peter Bloem's blog, with accompanying code on [GitHub](https://github.com/pbloem/former) and videos on [YouTube](https://www.youtube.com/playlist?list=PLIXJ-Sacf8u60G1TwcznBmK6rEL3gmZmV). Note that Peter Bloem uses positional _embeddings_ rather than encodings. The drawback with embeddings is that each possible sequence length needs to be seen during training, or that length will not be learned by the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rytwZE2w7R8F"
      },
      "source": [
        "#### Implement the function for positional encodings\n",
        "Implementation of the function proposed in the paper \"Attention Is All You Need\" by Vaswani et al. (2017).\n",
        "\n",
        "$PE_{(pos, 2i)} = sin(\\frac{pos}{10000^{2i/d_{model}}})$\n",
        "\n",
        "$PE_{(pos, 2i+1)} = cos(\\frac{pos}{10000^{2i/d_{model}}})$\n",
        "\n",
        "Where $pos$ is the token position and $i$ is the embedding dimension (which matches the number of embeddings in the model).\n",
        "\n",
        "This creates a 2D matrix of $pos$ rows by $d_{model}$ columns. There are as many rows as the max_length parameter set when creating the positional encodings, and as many columns as there are embedding dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4PDyF7-7R8F"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding1D(nn.Module):\n",
        "    '''\n",
        "    Positional encodings for labels (text sequences),\n",
        "    which have a single dimension (tokens).\n",
        "\n",
        "    Adapted from Tobias van der Werff's GitHub repo:\n",
        "      full-page-handwriting-recognition\n",
        "    '''\n",
        "    def __init__(self, d_model, max_len=3500):\n",
        "        # Inherit from parent class (nn.Module)\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Compute positional encodings in logarithmic space\n",
        "        # This is a 2D matrix (tensor) with position (max_len) as the rows\n",
        "        # and model embedding dimension (d_model) as the columns\n",
        "        pe = torch.zeros((max_len, d_model), requires_grad=False)\n",
        "        \n",
        "        # A 2D tensor with max_len rows and 1 column. Each row holds the\n",
        "        # value of that position (from 0 to max_len)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        \n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
        "        )\n",
        "\n",
        "        # Indexing technique accesses all rows (sequence position),\n",
        "        # but skips every other column (embedding dimension)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Create a dimension for the batch.\n",
        "        # The dimensions of pe will now be (batch_size, max_len, d_model)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # Save the encodings in the model, without registering them as a parameter\n",
        "        # (a \"buffer\" is saved to the model, but not as a learnable paramater).\n",
        "        self.register_buffer(\"pos_enc\", pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Add a 1D positional encoding to an embedding tensor.\n",
        "\n",
        "        Args\n",
        "        ---\n",
        "        `x`: Tensor of shape (batch_size, num_tokens, d_model)\n",
        "            The embedding tensor to which positional encodings\n",
        "            will be added.\n",
        "        \n",
        "        Returns\n",
        "        ---\n",
        "        The sum of the input `x` (embedding tensor) with the positional\n",
        "        encodings.\n",
        "        '''\n",
        "        _, T, _ = x.shape\n",
        "        assert T <= self.pos_enc.size(1), (\n",
        "            f\"The given embedding has {T} tokens, which is more than the max length stored in the positional encodings ({self.max_len}). \"\n",
        "            + \"\\nPlease increase the 'max_len' argument in the `PositionalEncoding1D` instance so max_len is greater than the number of tokens in the input.\"\n",
        "        )\n",
        "\n",
        "        # Add positional encodings to the input tensor, across all batches\n",
        "        # and up to the number of rows as there are input tokens.\n",
        "        return x + self.pos_enc[:, :T]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WleBDap-7R8F"
      },
      "source": [
        "### 2D positional encoding (for encoder ResNet)\n",
        "\n",
        "Implementation reference: Tobias van der Werff's GitHub repo: [full-page-handwriting-recognition/src/models](https://github.com/tobiasvanderwerff/full-page-handwriting-recognition/blob/47a2d27fc40815898474e9f74badc7d544740fee/src/models.py#L56)\n",
        "\n",
        "p. 7 of Singh et al. (2021) states:\n",
        "\n",
        ">    \"The encoder uses a CNN to extract a 2D feature-map from the input image.\n",
        "    It uses the ResNet architecture without its last two layers: \n",
        "    the average-pool and linear projection. The feature-map is then projected \n",
        "    to match the Transformer's hidden-size dmodel, then a 2D positional encoding \n",
        "    added and finally flattened into a 1D sequence. 2D positional encoding is a \n",
        "    fixed sinusoidal encoding as in Vaswani et al. (2017), but using the first \n",
        "    dmodel/2 channels to encode the Y coordinate and the rest to encode the \n",
        "    X coordinate (similar to Parmar et al. (Image Transformer, 2018)).\n",
        "    Output I of the Flatten layer is made available to all Transformer decoder layers,\n",
        "    as is standard.\"\n",
        "\n",
        "$PE_{(y, 2i)} = sin(\\frac{y}{10000^{2i/d_{model}}})$\n",
        "\n",
        "$PE_{(y, 2i+1)} = cos(\\frac{y}{10000^{2i/d_{model}}})$\n",
        "\n",
        "$PE_{(x, \\frac{d_{model}}{2} + 2i)} = sin(\\frac{x}{10000^{2i/d_{model}}})$\n",
        "\n",
        "$PE_{(x, \\frac{d_{model}}{2} + 2i + 1)} = cos(\\frac{x}{10000^{2i/d_{model}}})$\n",
        "\n",
        "Where $x$ is the x-coordinate in the feature map, $y$ is the y-coordinate in the feature map, and $i$ is the embedding dimension (which matches the embedding dimensions of the model).\n",
        "\n",
        "This creates a 3D matrix (tensor) of $y$ rows by $x$ columns by $d_{model}$ channels. There are as many rows as the height (in pixels) of the feature map output by the encoder CNN, as many columns as the width (in pixels) of the feature map, and as many channels as there are embedding dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLyDKose7R8F"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding2D(nn.Module):\n",
        "    '''\n",
        "    Positional encodings for inputs (feature maps),\n",
        "    which have three dimensions (channels, height, width).\n",
        "\n",
        "    The feature maps are the final output from the Convolutional\n",
        "    Neural Network used as an image encoder, such as the ResNet \n",
        "    models used in this implementation.\n",
        "\n",
        "    p. 7 of Singh et al. (2021) states:\n",
        "\n",
        "    \"The encoder uses a CNN to extract a 2D feature-map from the input image.\n",
        "    It uses the ResNet architecture without its last two layers: \n",
        "    the average-pool and linear projection. The feature-map is then projected \n",
        "    to match the Transformer's hidden-size dmodel, then a 2D positional encoding \n",
        "    added and finally flattened into a 1D sequence. 2D positional encoding is a \n",
        "    fixed sinusoidal encoding as in Vaswani et al. (2017), but using the first \n",
        "    dmodel/2 channels to encode the Y coordinate and the rest to encode the \n",
        "    X coordinate (similar to Parmar et al. (Image Transformer, 2018)).\n",
        "    Output I of the Flatten layer is made available to all Transformer decoder layers,\n",
        "    as is standard.\"\n",
        "\n",
        "    Adapted from Tobias van der Werff's GitHub repo:\n",
        "      full-page-handwriting-recognition\n",
        "    '''\n",
        "    def __init__(self, d_model, max_len=100):\n",
        "        '''\n",
        "\n",
        "        Args\n",
        "        ---\n",
        "        d_model: int\n",
        "            the number of embedding dimensions in the model\n",
        "\n",
        "        max_len: int\n",
        "            the maximum size (in pixels) for the height and width\n",
        "            of the feature map output from the encoder CNN\n",
        "        '''\n",
        "        # Inherit from parent class (nn.Module)\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_len = max_len\n",
        "\n",
        "        assert d_model % 4 == 0, f\"Model dimensions must be divisible by 4. `d_model` is {d_model}, which is not divisible by 4.\"\n",
        "\n",
        "        # Initialize positional encodings.\n",
        "        # These are 2D tensors of max_len rows and (d_model / 2) columns.\n",
        "        pe_x = torch.zeros((max_len, d_model // 2), requires_grad=False)\n",
        "        pe_y = torch.zeros((max_len, d_model //2), requires_grad=False)\n",
        "\n",
        "        # A 2D tensor with max_len rows and 1 column. Each row holds the\n",
        "        # value of that position (from 0 to max_len)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        \n",
        "        # === COMMENT FROM TOBIAS van der WERFF ===\n",
        "        # Div term term is calculated in log space, as done in other implementations;\n",
        "        # this is most likely for numerical stability (i.e., precision). The expression below is\n",
        "        # equivalent to:\n",
        "        #     div_term = 10000 ** (torch.arange(0, d_model // 2, 2) / d_model)\n",
        "        # === END ===\n",
        "        \n",
        "        div_term = torch.exp(\n",
        "            -math.log(10000.0) * torch.arange(0, d_model // 2, 2) / d_model\n",
        "        )\n",
        "\n",
        "        # Calculate the positional encodings\n",
        "        # Indexing technique accesses all rows (y or x coordinate),\n",
        "        # but skips every other column (embedding dimension).\n",
        "        # Shape is (max_len, d_model / 4)\n",
        "        pe_y[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe_y[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe_x[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe_x[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Save the encodings in the model's state, without registering them as a parameter\n",
        "        # (a \"buffer\" is saved to the model, but not as a learnable paramater).\n",
        "        self.register_buffer(\"pos_enc_y\", pe_y)\n",
        "        self.register_buffer(\"pos_enc_x\", pe_x)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Add a 2D positional encoding to an embedding tensor.\n",
        "\n",
        "        Args\n",
        "        ---\n",
        "        `x`: Tensor of shape (batch_size, width, height, d_model)\n",
        "            The embedding tensor to which positional encodings\n",
        "            will be added.\n",
        "        \n",
        "        Returns\n",
        "        ---\n",
        "        The sum of the input `x` (embedding tensor) with the positional\n",
        "        encodings.\n",
        "        '''\n",
        "        _, w, h, _ = x.shape\n",
        "        assert w <= self.pos_enc_x.size(0) and h <= self.pos_enc_y.size(0), (\n",
        "            \"The stored positional encodings do not have enough dimensions to support the input feature map.\"\n",
        "            + f\"\\nInput feature map must be of dimensions less than or equal to {self.pos_enc_x.size(0)} width by {self.pos_enc_x.size(0)} height, \"\n",
        "            + f\"\\nbut the input provided has dimensions {w} width by {h} height.\"\n",
        "            + f\"Please re-initialize the `PositionalEncoding2D` instance and set the 'max_len' argument greater than or equal to {max(w, h)}.\" \n",
        "        )\n",
        "\n",
        "        # Add positional encodings to the input tensor, across all d_model dimensions (columns)\n",
        "        # and up to the number of rows as there are input dimensions (width and height of input feature map).\n",
        "        pe_x_ = self.pos_enc_x[:w, :].unsqueeze(1).expand(-1, h, -1)    # shape: (w, h, d_model / 2)\n",
        "        pe_y_ = self.pos_enc_y[:h, :].unsqueeze(0).expand(w, -1, -1)    # shape: (w, h, d_model / 2)\n",
        "        # Combine across the channel dimension (z-axis of the 3D tensor)\n",
        "        pe = torch.cat([pe_y_, pe_x_], -1)                              # shape: (w, h, d_model)\n",
        "        # Unsqueeze to accommodate batch dimension\n",
        "        pe = pe.unsqueeze(0)                                            # shape: (1, w, h, d_model)\n",
        "        \n",
        "        return x + pe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLAij1H77R8G"
      },
      "source": [
        "## ResNet encoder\n",
        "Adapted from: Tobias van der Werff's GitHub repo: [full-page-handwriting-recognition/src/models](https://github.com/tobiasvanderwerff/full-page-handwriting-recognition/blob/47a2d27fc40815898474e9f74badc7d544740fee/src/models.py#L250)\n",
        "\n",
        "ResNet is included as one of the models in [`torchvision.models`](https://pytorch.org/vision/stable/models.html). The source code is found in the [PyTorch Torchvision GitHub repo](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2X2Yc2w7R8G"
      },
      "outputs": [],
      "source": [
        "class ResNetEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Takes as input grayscale (single-channel) images\n",
        "    and outputs an encoding of the image that the decoder\n",
        "    attends over to produce the sequential output (text).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, model_name: str, dropout: float):\n",
        "        \"\"\"\n",
        "        Initialize the encoder part of the network.\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        `d_model`: int\n",
        "            The number of dimensions in the model's embedding space.\n",
        "            Singh et al. (p. 9) used d_model=260.\n",
        "        `model_name`: {'resnet18', 'resnet34', 'resnet50'}\n",
        "            The model to use for the encoder. PyTorch supports a number\n",
        "            of ResNet architectures, but the three implemented here are\n",
        "            the ones used by Singh et al. (base model was resnet34)\n",
        "        `dropout`: float\n",
        "            The percentage of neurons to mask during dropout on the final\n",
        "            layer of the encoder.\n",
        "        \"\"\"\n",
        "        # Inherit the methods of nn.Module\n",
        "        super().__init__()\n",
        "\n",
        "        # Check that inputs are specified correctly\n",
        "        assert d_model % 4 == 0, f\"Model embedding dimension ('d_model') must be divisible by 4. Provided value was: {d_model}.\"\n",
        "        _models = ['resnet18', 'resnet34', 'resnet50']\n",
        "        err_message = f\"{model_name} is not one of the available models: {_models}\"\n",
        "        assert model_name in _models, err_message\n",
        "\n",
        "        # Save attributes\n",
        "        self.d_model = d_model\n",
        "        self.model_name = model_name\n",
        "        self.pos_encoding = PositionalEncoding2D(d_model)\n",
        "        self.drop = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Load the resnet model from torchvision.models with random weights\n",
        "        resnet = getattr(torchvision.models, model_name)(pretrained=False)\n",
        "        modules = list(resnet.children())\n",
        "\n",
        "        # Modify the first convolutional layer so its input takes a single-channel image\n",
        "        # (i.e., grayscale rather than RGB)\n",
        "        conv1 = modules[0]\n",
        "        conv1 = nn.Conv2d(\n",
        "            in_channels = 1,\n",
        "            out_channels = conv1.out_channels,\n",
        "            kernel_size = conv1.kernel_size,\n",
        "            stride = conv1.stride,\n",
        "            padding = conv1.padding,\n",
        "            bias = conv1.bias\n",
        "        )\n",
        "\n",
        "        # Create the encoder module by combining conv1 with the rest of the ResNet\n",
        "        # except the last two layers, which will be replaced by 1x1 conv and positional encoding.\n",
        "        # The final two layers of ResNet are avgpool and fully-connected linear. See: https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L203\n",
        "        self.encoder = nn.Sequential(conv1, *modules[1:-2])\n",
        "        # Define the linear layer, a 1x1 convolution that takes as input the same number of features\n",
        "        # as the final ResNet fully-connected layer (which we removed in the line above)\n",
        "        self.linear = nn.Conv2d(in_channels=resnet.fc.in_features, out_channels=d_model, kernel_size=1)\n",
        "\n",
        "\n",
        "    def forward(self, imgs):\n",
        "        x = self.encoder(imgs)                              # x shape: (batch_size, d_model, w, h)\n",
        "        x = self.linear(x).transpose(1, 2).transpose(2, 3)  # x shape: (batch_size, w, h, d_model)\n",
        "        x = self.pos_encoding(x)                            # x shape: (batch_size, w, h, d_model)\n",
        "        x = self.drop(x)                                    # x shape: (batch_size, w, h, d_model)\n",
        "        x = x.flatten(1, 2)                                 # x shape: (batch_size, w*h, d_model)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tpo0kt9S7R8H"
      },
      "source": [
        "## Transformer decoder\n",
        "In \"[Full Page Handwriting Recognition via Sequence-to-Sequence Extraction](https://paperswithcode.com/paper/full-page-handwriting-recognition-via-image)\" (2021), Singh et al. describe the hyperparameters they used for the transformer decoder on pages 8-9 (section 4: Training Configuration and Procedure).\n",
        "\n",
        "PyTorch has a built-in module [`torch.nn.TransformerDecoder`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html) for creating decoder-only Transformer networks.\n",
        "\n",
        "This is not to be confused with the `torch.nn.Transformer` module which implements the full encoder-decoder Transformer from the paper \"Attention is All You Need\" by Vaswani et al. (2017).\n",
        "\n",
        "Adapted from: Tobias van der Werff's GitHub repo: [full-page-handwriting-recognition -> models.py](https://github.com/tobiasvanderwerff/full-page-handwriting-recognition/blob/47a2d27fc40815898474e9f74badc7d544740fee/src/models.py#L107)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8aCT8gZ7R8H"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    # Class-level variables (with type annotations)\n",
        "    decoder: nn.TransformerDecoder\n",
        "    clf: nn.Linear\n",
        "    emb: nn.Embedding\n",
        "    pos_enc: PositionalEncoding1D\n",
        "    drop: nn.Dropout\n",
        "    \n",
        "    vocab_len: int\n",
        "    max_seq_len: int\n",
        "    eos_token_idx: int\n",
        "    sos_token_idx: int\n",
        "    pad_token_idx: int\n",
        "    d_model: int\n",
        "    num_layers: int\n",
        "    num_heads: int\n",
        "    dim_feedforward: int\n",
        "    dropout: float\n",
        "    activation_fn: str\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_len: int,\n",
        "        max_seq_len: int,\n",
        "        eos_token_idx: int,\n",
        "        sos_token_idx: int,\n",
        "        pad_token_idx: int,\n",
        "        d_model: int,\n",
        "        num_layers: int,\n",
        "        num_heads: int,\n",
        "        dim_feedforward: int,\n",
        "        dropout: float,\n",
        "        activation_fn: str = 'gelu'\n",
        "    ):\n",
        "        # Inherit methods from the base class (nn.Module)\n",
        "        super().__init__()\n",
        "        assert d_model % 4 == 0, f\"Model dimensions must be divisible by 4. `d_model` is {d_model}, which is not divisible by 4.\"\n",
        "\n",
        "        self.vocab_len = vocab_len\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.eos_token_idx = eos_token_idx\n",
        "        self.sos_token_idx = sos_token_idx\n",
        "        self.pad_token_idx = pad_token_idx\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.num_heads = num_heads\n",
        "        self.dim_feedforward = dim_feedforward\n",
        "        self.dropout = dropout\n",
        "        self.activation_fn = activation_fn\n",
        "\n",
        "        self.emb = nn.Embedding(vocab_len, d_model)\n",
        "        self.pos_enc = PositionalEncoding1D(d_model)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model = d_model,\n",
        "            nhead = num_heads,\n",
        "            dim_feedforward = dim_feedforward,\n",
        "            dropout = dropout,\n",
        "            activation = activation_fn,\n",
        "            batch_first = True\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "        self.clf = nn.Linear(d_model, vocab_len)\n",
        "        self.drop = nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, memory: torch.Tensor):\n",
        "        '''\n",
        "        Takes an input Tensor and pushes it through the Transformer decoder,\n",
        "        softmaxing over the logits output from the final feed-forward layer\n",
        "        (which creates a probability distribution over the vocabulary),\n",
        "        then selects the vocabulary token with the highest probability.\n",
        "\n",
        "        In other words, this uses \"greedy decoding\" rather than \"beam search,\"\n",
        "        since only the top-probability value is sampled for each sequence iteration.\n",
        "        '''\n",
        "        B, _, _ = memory.shape\n",
        "        # Info on logits: real-valued numbers that represent probabilities and\n",
        "        # are inputs to the softmax function to get normalized (sum to 1) probabilities\n",
        "        # https://en.wikipedia.org/wiki/Logit\n",
        "        # https://deepai.org/machine-learning-glossary-and-terms/logit\n",
        "        # https://datascience.stackexchange.com/questions/31041/what-does-logits-in-machine-learning-mean/31045#31045\n",
        "        # https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow\n",
        "        all_logits = []\n",
        "\n",
        "        # Set start-of-sequence tokens for each item in the batch\n",
        "        sampled_ids = [torch.full(size=[B], fill_value=self.sos_token_idx).to(memory.device)]\n",
        "        \n",
        "        # target's shape is: (batch_size, 1, d_model)\n",
        "        # Multiplying by the square root of d_model is a more efficient way\n",
        "        # to scale the self-attention, rather than allowing the numbers to get\n",
        "        # larger and scaling them later on. See the Peter Bloem blog article\n",
        "        # referenced in a preceeding section.\n",
        "        target = self.pos_enc(\n",
        "            self.emb(sampled_ids[0]).unsqueeze(1) * math.sqrt(self.d_model)\n",
        "        )\n",
        "        target = self.drop(target)\n",
        "\n",
        "        # Set each item in the batch to \"end-of-sequence not reached\"\n",
        "        eos_sampled = torch.zeros(B).bool()\n",
        "\n",
        "        for token in range(self.max_seq_len):\n",
        "            target_mask = self.subsequent_mask(size=len(sampled_ids)).to(memory.device)\n",
        "            out = self.decoder(target, memory, tgt_mask=target_mask)    # shape: (batch_size, tokens, d_model)\n",
        "            # Grab the output from the final feed-forward layer for the last\n",
        "            #   token (across all batches and all d_model dimensions)\n",
        "            logits = self.clf(out[:, -1, :])                            # shape: (batch_size, vocab_size)\n",
        "            _, pred = torch.max(input=logits, dim=-1)\n",
        "            all_logits.append(logits)\n",
        "            sampled_ids.append(pred)\n",
        "            # Check if all items in the batch have output end-of-sequence tokens\n",
        "            for i, pr in enumerate(pred):\n",
        "                if pr == self.eos_token_idx:\n",
        "                    eos_sampled[i] = True\n",
        "            if eos_sampled.all():\n",
        "                # Exit loop, all items in the batch have predicted end-of-sequence tokens\n",
        "                break\n",
        "            # Add position encodings to the (scaled) embeddings of the predicted tokens\n",
        "            target_ext = self.drop(\n",
        "                self.pos_enc.pos_enc[:, len(sampled_ids)-1] + self.emb(pred) * math.sqrt(self.d_model)\n",
        "            ).unsqueeze(1)\n",
        "            # Concatenate along dimension 1 (tokens)\n",
        "            target = torch.cat([target, target_ext], 1)\n",
        "        \n",
        "        # torch.stack concatenates a list of tensors along a new dimension\n",
        "        sampled_ids = torch.stack(sampled_ids, dim=1)\n",
        "        all_logits = torch.stack(all_logits, dim=1)\n",
        "\n",
        "        # Replace all tokens after an EOS token with the pad token\n",
        "        eos_indexes = (sampled_ids == self.eos_token_idx).float().argmax(dim=1)\n",
        "        for batch_item in range(B):\n",
        "            if eos_indexes[batch_item] != 0:\n",
        "                # the sampled sequence has an EOS token,\n",
        "                # so set all tokens after the EOS token to the pad token\n",
        "                sampled_ids[batch_item, eos_indexes + 1:] = self.pad_token_idx\n",
        "        \n",
        "        return all_logits, sampled_ids\n",
        "\n",
        "    \n",
        "    def decode_teacher_forcing(self, memory: torch.Tensor, target: torch.Tensor):\n",
        "        '''\n",
        "        Implements attention over the ResNet encoder's output.\n",
        "        \"Teacher forcing\" refers to the autoregressive self-supervised technique\n",
        "        of having the targets set to the inputs shifted one token to the right\n",
        "        (that is, the targets are the next token in the input sequence).\n",
        "\n",
        "        Returns a Tensor with a probability distribution (actually, logits, one\n",
        "        step before a softmaxed probability distribution, but the same results\n",
        "        when just choosing the max value as the predicted token) over the vocabulary,\n",
        "        with dimensions: (batch_size, tokens, vocab_len)\n",
        "\n",
        "        Args\n",
        "        ---\n",
        "        `memory`: a Tensor of shape (batch_size, w*h, d_model)\n",
        "            This is the tensor output by the encoder model. It is used\n",
        "            as the Key and Value vectors for attention.\n",
        "        `target`: a Tensor of shape (batch_size, tokens)\n",
        "            The target tokens being predicted.\n",
        "        '''\n",
        "        B, T = target.shape\n",
        "\n",
        "        # Shift elements of target to the right, for predicting\n",
        "        # the next item in the sequence (teacher forcing)\n",
        "        target = torch.cat(\n",
        "            [\n",
        "                torch.full([B], fill_value=self.sos_token_idx).unsqueeze(1).to(memory.device),\n",
        "                target[:, :-1]\n",
        "            ],\n",
        "            dim=1\n",
        "        )\n",
        "\n",
        "        # Implement masking for causal self-attention (where all future tokens\n",
        "        # cannot be used in the prediction).\n",
        "        # == COMMENT FROM TOBIAS VAN DER WERFF ==\n",
        "        # This is a combination of pad token masking (target_key_padding_mask)\n",
        "        # and causal self-attention masking (target_mask), where the target_mask\n",
        "        # is of shape (T, T) and the targets are shifted to the right by one.\n",
        "        # == END OF COMMENT FROM TOBIAS VAN DER WERFF ==\n",
        "        # The next line first initializes a variable called target_key_padding_mask,\n",
        "        # then makes that variable equal the Tensor target, and then checks whether\n",
        "        # each item in the target_key_padding_mask equals the pad_token_idx,\n",
        "        # returning Boolean values for each item in the Tensor.\n",
        "        target_key_padding_mask = target == self.pad_token_idx\n",
        "        target_mask = self.subsequent_mask(size=T).to(target.device)\n",
        "\n",
        "        target = self.pos_enc(self.emb(target) * math.sqrt(self.d_model))\n",
        "        target = self.drop(target)\n",
        "        out = self.decoder(\n",
        "            target, memory, tgt_mask=target_mask, tgt_key_padding_mask=target_key_padding_mask\n",
        "        )\n",
        "        logits = self.clf(out)\n",
        "        return logits\n",
        "\n",
        "\n",
        "    # The @staticmethod decorator places a standalone function within the\n",
        "    # scope of a class, to show that the function is somehow related to\n",
        "    # the class, even though the function does not require the class nor\n",
        "    # any instantiated class object to perform its operations.\n",
        "    # See: https://stackoverflow.com/questions/23508248/why-do-we-use-staticmethod\n",
        "    @staticmethod\n",
        "    def subsequent_mask(size: int):\n",
        "        mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
        "        return mask == 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO0uvDa07R8I"
      },
      "source": [
        "## Complete network\n",
        "\n",
        "Adapted from: Tobias van der Werff's GitHub repo: [full-page-handwriting-recognition -> models.py](https://github.com/tobiasvanderwerff/full-page-handwriting-recognition/blob/47a2d27fc40815898474e9f74badc7d544740fee/src/models.py#L304).\n",
        "\n",
        "For the model's hyperparameters, see [pgs. 8-9 of the PDF](https://arxiv.org/pdf/2103.06450v2.pdf) for \"Full Page Handwritten Text Recognition via Image to Sequence Extraction\" by Singh et al. (2021)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_Qu3UZt7R8I"
      },
      "outputs": [],
      "source": [
        "# from typing import Callable, Optional\n",
        "\n",
        "class HTRmodel(nn.Module):\n",
        "    # Class-level variables with type annotations\n",
        "    encoder: ResNetEncoder\n",
        "    decoder: TransformerDecoder\n",
        "    cer_metric: CharacterErrorRate\n",
        "    loss_fn: Callable\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        max_seq_len: int = 3500,\n",
        "        d_model: int = 260,\n",
        "        num_layers: int = 6,\n",
        "        num_heads: int = 4,\n",
        "        dim_feedforward: int = 1024,\n",
        "        encoder_name: str = 'resnet18',\n",
        "        drop_enc: float = 0.5,\n",
        "        drop_dec: float = 0.5,\n",
        "        activation_dec: str = 'gelu',\n",
        "        label_smoothing: float = 0.0,\n",
        "        vocab_len: Optional[int] = None,\n",
        "        vocab_dict: dict = token_to_index\n",
        "    ):\n",
        "        '''\n",
        "        A complete neural network for full-page handwritten text recognition.\n",
        "        Incorporates a convolutional neural network encoder (`ResNetEncoder`)\n",
        "        with a Transformer decoder (`TransformerDecoder`). The encoder takes a\n",
        "        grayscale (or binary) image as input and outputs a feature map. Each layer\n",
        "        in the decoder attends over the feature map output from the encoder, along\n",
        "        with masked self-attention on previous output tokens.\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        max_seq_len: int = 3500,\n",
        "        d_model: int = 260,\n",
        "        num_layers: int = 6,\n",
        "        num_heads: int = 4,\n",
        "        dim_feedforward: int = 1024,\n",
        "        encoder_name: str = 'resnet18',\n",
        "        drop_enc: float = 0.5,\n",
        "        drop_dec: float = 0.5,\n",
        "        activation_dec: str = 'gelu',\n",
        "        label_smoothing: float, default=0.0\n",
        "            The label-smoothing epsilon setting for cross-entropy loss.\n",
        "            0 means no smoothing.\n",
        "        vocab_len: Optional[int] = None\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "        # Get the indices for the special tokens\n",
        "        self.sos_token_idx = vocab_dict['<START>']\n",
        "        self.eos_token_idx = vocab_dict['<END>']\n",
        "        self.pad_token_idx = vocab_dict['<PAD>']\n",
        "\n",
        "        # Initialize encoder and decoder\n",
        "        self.encoder = ResNetEncoder(\n",
        "            d_model=d_model, model_name=encoder_name, dropout=drop_enc\n",
        "        )\n",
        "\n",
        "        self.decoder = TransformerDecoder(\n",
        "            vocab_len = vocab_len,\n",
        "            max_seq_len = max_seq_len,\n",
        "            eos_token_idx = self.eos_token_idx,\n",
        "            sos_token_idx = self.sos_token_idx,\n",
        "            pad_token_idx = self.pad_token_idx,\n",
        "            d_model = d_model,\n",
        "            num_layers = num_layers,\n",
        "            num_heads = num_heads,\n",
        "            dim_feedforward = dim_feedforward,\n",
        "            dropout = drop_dec,\n",
        "            activation_fn = activation_dec\n",
        "        )\n",
        "\n",
        "        # Initialize metrics and loss function\n",
        "        self.cer_metric = CharacterErrorRate()\n",
        "        self.loss_fn = nn.CrossEntropyLoss(\n",
        "            ignore_index=self.pad_token_idx, label_smoothing=label_smoothing\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, imgs, targets=None):\n",
        "        '''\n",
        "        Run inference on the model using greedy decoding,\n",
        "        that is, return predicted text from the input `imgs`.\n",
        "\n",
        "        Returns a 3-tuple of (logits, predictions, loss)\n",
        "        - logits: represents the probability distribution\n",
        "            output across the vocabulary space\n",
        "        - predictions: argmax of the logits at each sequential step\n",
        "        - loss: if  `targets` is not None, this value holds the loss\n",
        "            of predictions vs. targets\n",
        "        \n",
        "        Parameters\n",
        "        ---\n",
        "        imgs: Tensor\n",
        "            A Tensor of shape: (batch_size, w, h)\n",
        "        targets: Tensor\n",
        "            A Tensor of shape (batch_size, tokens)\n",
        "        '''\n",
        "        # Encoder input dims: (batch_size, d_model, w, h)\n",
        "        # Decoder input dims: (batch_size, w*h, d_model)\n",
        "        logits, predictions = self.decoder(self.encoder(imgs))\n",
        "        # logits dims: (batch_size, tokens, vocab_len), \n",
        "        #  where tokens is the max_len, with padding added to tokens that come\n",
        "        #  after the <END> token.\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # print(f\"Target shape: {targets.shape}\")\n",
        "            # print(f\"Logits shape: {logits.shape}\")\n",
        "            # print(f\"Predictions shape: {predictions.shape}\")\n",
        "            # # Target shape: torch.Size([1, 3500])\n",
        "            # # Logits shape: torch.Size([1, 3500, 116])\n",
        "            # # Predictions shape: torch.Size([1, 3500])\n",
        "            loss = self.loss_fn(\n",
        "                logits[:, :targets.size(1), :].transpose(1, 2),\n",
        "                targets[:, :logits.size(1)]\n",
        "            )\n",
        "        return logits, predictions, loss\n",
        "\n",
        "\n",
        "    def forward_teacher_forcing(\n",
        "        self, imgs: torch.Tensor, targets: torch.Tensor):\n",
        "        '''\n",
        "        Predict text using teacher forcing and greedy decoding.\n",
        "\n",
        "        Teacher forcing means that the model expects as input\n",
        "        the ground truth tokens.\n",
        "\n",
        "        Returns:\n",
        "        - logits: reflects a probability distribution across\n",
        "            output classes (that is, the model's character-level\n",
        "            vocabulary)\n",
        "        - loss: value measuring the model's loss\n",
        "        '''\n",
        "        memory = self.encoder(imgs)\n",
        "        logits = self.decoder.decode_teacher_forcing(memory, targets)\n",
        "        loss = self.loss_fn(logits.transpose(1, 2), targets)\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def calculate_char_error(self, predictions, targets):\n",
        "        self.cer_metric.reset()\n",
        "        cer = self.cer_metric(predictions, targets)\n",
        "        return cer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoOTnw417R8L"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIdtbP0I7R8M"
      },
      "source": [
        "### Set training variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOA4xAPn_8Ms"
      },
      "outputs": [],
      "source": [
        "# Create file to save model performance metrics\n",
        "if not os.path.exists('./model_performance.csv'):\n",
        "    with open('./model_performance.csv', mode='a', encoding='UTF-8') as csv_file:\n",
        "        csv_file.write(','.join(\n",
        "            ['Batch number', 'Training loss', 'Validation loss', 'Validation error']) \n",
        "            + '\\n')\n",
        "\n",
        "# Create .csv file to save checkpoint performance info\n",
        "if not os.path.exists('./checkpoint_comparisons.csv'):\n",
        "    with open('./checkpoint_comparisons.csv', mode='a', encoding='UTF-8') as chkpt_file:\n",
        "        chkpt_file.write(','.join(\n",
        "            ['Batch number', 'Loss', 'Character error rate'])\n",
        "            + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_sbLqg77R8P"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "#### Saving checkpoints\n",
        "To save periodic checkpoints, create a checkpoint dictionary with at least the `model.state_dict()` and `optimizer.state_dict()` as entries in the dictionary. You may also want to include entries for `epoch`, `loss`, or `nn.Embedding()` layers that occur outside of the main `model`. Next, use `torch.save(checkpoint_dict, filepath)` with a `.pt` or `.pth` extension in `filepath` and the saved file will be a serialized dictionary that holds all the information you wanted to capture.\n",
        "\n",
        "To load the saved checkpoint, use `checkpoint = torch.load(checkpoint_filepath)`, then intitialize the model and optimizer as normal, then do `model.load_state_dict(checkpoint['model_state_dict'])` to load the parameters and registered buffers (like BatchNorm mean) into the model. Similarly, run `optimizer.load_state_dict(checkpoint['optim_state_dict'])` to load the optimizer's parameters into the initialized optimizer instance. Notice how the saved `state_dict()` is simply a value in a dictionary that is accessed with the keys 'model_state_dict' or 'optim_state_dict'. The `torch.load()` function takes the saved checkpoint file and loads it back into a Python dictionary.\n",
        "\n",
        "**References**\n",
        "* [PyTorch tutorial: Saving and loading models](https://pytorch.org/tutorials/beginner/saving_loading_models.html), most detailed, with helpful information on what a state dictionary is, how to configure a checkpoint dictionary, and a reminder to use `model.eval()` to set the model in inference mode if you plan to use the saved model for predictions, since by default it is saved in `model.train()` mode.\n",
        "* [Weights and Biases blog: Saving checkpoints in PyTorch](https://wandb.ai/wandb/common-ml-errors/reports/How-to-Save-and-Load-Models-in-PyTorch--VmlldzozMjg0MTE), quick and helpful reference with all key information included.\n",
        "* [PyTorch recipes: Saving and loading a general checkpoint](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html), takes a slightly different approach and clearly demonstrates the checkpoint dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjTf3u8RW21g"
      },
      "outputs": [],
      "source": [
        "# def initialize(\n",
        "#     batch_size = 4,\n",
        "#     vocab_dict = token_to_index,\n",
        "#     device = device):\n",
        "#     \"\"\"\n",
        "#     Initialize the datasets, dataloaders, neural network (model), \n",
        "#     and optimizer (Adam).\n",
        "\n",
        "#     Returns a tuple of the format:\n",
        "#         (train_loader, val_loader, model, optimizer)\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Intialize the training and validation datasets\n",
        "#     train_dataset = FPHRdataset(\n",
        "#         path_to_image_folder = r'/content/drive/MyDrive/School/Deep learning final project/training_data/processed_images',\n",
        "#         path_to_label_folder = r'/content/drive/MyDrive/School/Deep learning final project/training_data/labels',\n",
        "#         max_width = 2300,\n",
        "#         max_height = 1900,\n",
        "#         augmentation_likelihood = 0.5,\n",
        "#         train = True,\n",
        "#         vocab_dict = vocab_dict\n",
        "#     )\n",
        "\n",
        "#     val_dataset = FPHRdataset(\n",
        "#         path_to_image_folder = r'/content/drive/MyDrive/School/Deep learning final project/training_data/processed_images',\n",
        "#         path_to_label_folder = r'/content/drive/MyDrive/School/Deep learning final project/training_data/labels',\n",
        "#         max_width = 2300,\n",
        "#         max_height = 1900,\n",
        "#         augmentation_likelihood = 0.5,\n",
        "#         train = False,\n",
        "#         vocab_dict = vocab_dict\n",
        "#     )\n",
        "\n",
        "#     # ==============================\n",
        "#     # Initialize data loaders\n",
        "#     # ==============================\n",
        "#     # Shuffle can be set to False to save memory. See: https://medium.com/@raghadalghonaim/memory-leakage-with-pytorch-23f15203faa4\n",
        "#     # Note that pin_memory cannot be set to True, or an exception will raise that says the Tensors must be dense float Tensors.\n",
        "#     train_loader = torch.utils.data.DataLoader(\n",
        "#         train_dataset, batch_size=batch_size, shuffle=True)\n",
        "#     val_loader = torch.utils.data.DataLoader(\n",
        "#         val_dataset, batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "#     # ==============================\n",
        "#     # Initalize model and optimizer\n",
        "#     # ==============================\n",
        "\n",
        "#     # Create the model and place it on the available device (CPU or GPU)\n",
        "#     model = HTRmodel(\n",
        "#         max_seq_len = 3500,\n",
        "#         d_model = 260,\n",
        "#         num_layers = 6,\n",
        "#         num_heads = 4,\n",
        "#         dim_feedforward = 1024,\n",
        "#         encoder_name = 'resnet18',\n",
        "#         drop_enc = 0.2,\n",
        "#         drop_dec = 0.2,\n",
        "#         activation_dec = 'gelu',\n",
        "#         label_smoothing = 0,\n",
        "#         vocab_len = len(vocab_dict),\n",
        "#         vocab_dict = vocab_dict\n",
        "#     ).to(device)\n",
        "\n",
        "#     print(\"Model loaded. Total number of parameters: \"\n",
        "#         f\"{sum([layer.numel() for layer in model.parameters()]):,d}.\")\n",
        "\n",
        "#     # Initialize the optimizer.\n",
        "#     # Hyperparameters come from Singh et al., page 9.\n",
        "#     optim = torch.optim.Adam(\n",
        "#         model.parameters(), lr=0.0002, betas=(0.9, 0.999)\n",
        "#     )\n",
        "\n",
        "#     return train_loader, val_loader, model, optim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCBrxM7JdBxF"
      },
      "outputs": [],
      "source": [
        "# def process_one_batch(model):\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3Tqsr2iB7k0"
      },
      "outputs": [],
      "source": [
        "# def validate_one_batch(model):\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuR1vMtlCBtJ"
      },
      "outputs": [],
      "source": [
        "# def log_metrics(model):\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfwMqt_sCELr"
      },
      "outputs": [],
      "source": [
        "# def save_checkpoint(model):\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLkPwavJYpJT"
      },
      "outputs": [],
      "source": [
        "# def train_model_split_into_smaller_functions(\n",
        "#     num_epochs,\n",
        "#     batch_size = 4,\n",
        "#     save_every = 500,\n",
        "#     grad_accum_factor = 8,\n",
        "#     vocab_dict = token_to_index,\n",
        "#     device = device):\n",
        "#     \"\"\"\n",
        "#     Loop for training the neural network on full-page handwritten\n",
        "#     text recognition.\n",
        "\n",
        "#     Returns a tuple of lists of the format:\n",
        "#         `(batch_num, loss_train, loss_val, error_val)`,\n",
        "#     where\n",
        "#         `batch_num` is a list of batch numbers when model performance was logged\n",
        "#         `loss train` is a list of loss values during model training at each\n",
        "#             `batch_num`\n",
        "#         `loss_val` is a list of average loss values from the validation set,\n",
        "#             captured at each `batch_num`\n",
        "#         `error_val` is a list of the average character error rate from the\n",
        "#             validation set, captured at each `batch_num`\n",
        "#     \"\"\"\n",
        "    \n",
        "#     # Clear unused variables from memory\n",
        "#     gc.collect()\n",
        "\n",
        "#     # Create lists to store model performance information.\n",
        "#     batch_num = []\n",
        "#     loss_train = []\n",
        "#     loss_val = []\n",
        "#     error_val = []\n",
        "\n",
        "#     # Initialize the datasets, dataloaders, neural network, and optimizer\n",
        "#     train_loader, val_loader, model, optim = initialize(\n",
        "#         batch_size = batch_size,\n",
        "#         vocab_dict = vocab_dict,\n",
        "#         device = device)\n",
        "\n",
        "#     # Determine the total number of iterations (batches) that will be processed\n",
        "#     #  during training, for setting the 'total' argument of tqdm's progress bar\n",
        "    \n",
        "#     # Set number of iterations (batches) to use during validation loop.\n",
        "#     # len(val_loader) will use all available items in the validation set.\n",
        "#     # At the end of the validation loop, the loss and error values from each\n",
        "#     # batch will be averaged, returning an average loss and avg. error rate.\n",
        "#     val_iterations = 5      # much fewer than len(val_loader)\n",
        "    \n",
        "#     # Calculate total iterations (training plus validation)\n",
        "#     total_iterations = (\n",
        "#         (num_epochs * len(train_loader))\n",
        "#       + (num_epochs * ((len(train_loader) / save_every) * val_iterations))\n",
        "#     )\n",
        "\n",
        "#     iter_count = 0\n",
        "#     iters_per_epoch = len(train_loader)\n",
        "#     progress_bar = tqdm(total=total_iterations)\n",
        "#     progress_bar.set_description('Beginning model training')\n",
        "#     for epoch in range(num_epochs):\n",
        "#         # Process one batch of images at a time in a loop through\n",
        "#         #  all images in the training dataset.\n",
        "#         for imgs, lbls in train_loader:\n",
        "#             # Make sure the Tensors returned by the DataLoader are placed\n",
        "#             #  on the GPU to avoid running out of CPU memory.\n",
        "#             #  See: https://medium.com/@raghadalghonaim/memory-leakage-with-pytorch-23f15203faa4\n",
        "#             # Note that this step is unnecessary since I add the Tensors to the\n",
        "#             #  correct device in the __getitem__ function of my Dataset class.\n",
        "#             # imgs = imgs.to(device)\n",
        "#             # lbls = lbls.to(device)\n",
        "#             # Alternative version:\n",
        "#             # imgs = imgs.cuda(non_blocking=True)\n",
        "#             # lbls = lbls.cuda(non_blocking=True)\n",
        "            \n",
        "#             # Clear unused variables from memory\n",
        "#             gc.collect()\n",
        "\n",
        "#             iter_count += 1\n",
        "\n",
        "#             # Set BatchNorm and Dropout layers to training mode\n",
        "#             model.train()\n",
        "\n",
        "#             # Run data through model\n",
        "#             train_logits, train_loss = model.forward_teacher_forcing(imgs=imgs, targets=lbls)\n",
        "#             # Training predictions are not needed, so delete variable to save memory\n",
        "#             del train_logits\n",
        "\n",
        "#             # =========================\n",
        "#             # Update model parameters\n",
        "#             # =========================\n",
        "#             # See: https://stackoverflow.com/questions/62067400/understanding-accumulated-gradients-in-pytorch/62076913#62076913\n",
        "#             # Scale loss by the number of batches to accumulate\n",
        "#             loss = train_loss / grad_accum_factor\n",
        "#             # Backpropagate loss\n",
        "#             loss.backward()\n",
        "#             if iter_count % grad_accum_factor == 0:\n",
        "#                 # Update gradients\n",
        "#                 optim.step()\n",
        "#                 # Reset gradients if accumulation factor is reached\n",
        "#                 optim.zero_grad()\n",
        "\n",
        "#             # =========================\n",
        "#             # Run validation\n",
        "#             # =========================\n",
        "#             if iter_count % save_every == 0:\n",
        "#                 with torch.no_grad():\n",
        "#                     # Set model to inference mode so BatchNorm\n",
        "#                     # and Dropout layers perform consistently.\n",
        "#                     model.eval()\n",
        "#                     # Capture all losses and errors to average at the end.\n",
        "#                     val_losses = []\n",
        "#                     val_errors = []\n",
        "#                     # Loop through validation set.\n",
        "#                     for i, (val_imgs, val_lbls) in enumerate(val_loader):\n",
        "#                         val_logits, val_predictions, val_loss = model.forward(imgs=val_imgs, targets=val_lbls)\n",
        "#                         val_error = model.calculate_char_error(predictions=val_predictions, targets=val_lbls)\n",
        "#                         val_losses.append(val_loss.item())\n",
        "#                         val_errors.append(float(val_error))\n",
        "#                         progress_bar.set_description(\n",
        "#                             f\"Validation loop ({i+1}/{val_iterations}). \"\n",
        "#                             f\"Loss: {val_losses[-1]:,.2f}, \"\n",
        "#                             f\"Error: {val_errors[-1]:.1%} | \")\n",
        "#                         progress_bar.update(n=1)\n",
        "#                         progress_bar.refresh()\n",
        "#                         if i > val_iterations:\n",
        "#                             break\n",
        "#                 # Average the loss and error values\n",
        "#                 val_loss = np.mean(val_losses)\n",
        "#                 val_error = np.mean(val_errors)\n",
        "\n",
        "#                 # =========================\n",
        "#                 # Log model metrics\n",
        "#                 # =========================\n",
        "#                 batch_num.append(iter_count)\n",
        "#                 loss_train.append(train_loss.detach().item())\n",
        "#                 loss_val.append(val_loss)\n",
        "#                 error_val.append(val_error)\n",
        "#                 with open('./model_performance.csv', mode='a', encoding='UTF-8') as csv_file:\n",
        "#                     csv_file.write(','.join(\n",
        "#                         list(zip(batch_num, loss_train, loss_val, error_val))[-1])\n",
        "#                         + '\\n'\n",
        "#                     )\n",
        "#                 progress_bar.set_description(\n",
        "#                     f\"Resuming training. \"\n",
        "#                     f\"Epoch: {epoch+1}/{num_epochs}, \"\n",
        "#                     f\"Avg. val. loss: {loss_val[-1]:,.2f}, \"\n",
        "#                     f\"Avg. val. error: {error_val[-1]:.1%} | \")\n",
        "\n",
        "#                 # =========================\n",
        "#                 # Save checkpoint\n",
        "#                 # =========================\n",
        "#                 checkpoint_dict = {\n",
        "#                     'model_state': model.state_dict(),\n",
        "#                     'optimizer_state': optim.state_dict(),\n",
        "#                     'epoch_num': epoch + 1,\n",
        "#                     'loss': loss_val[-1]\n",
        "#                 }\n",
        "#                 checkpoint_filepath = f\"./checkpoint_{str(iter_count).zfill(5)}.pt\"\n",
        "#                 torch.save(checkpoint_dict, checkpoint_filepath)\n",
        "\n",
        "#                 # Save model accuracy\n",
        "#                 # model_info = f\"Epoch: {str(epoch+1).zfill(2)}, loss: {loss_val[-1]:,.3f}, character error rate: {error_val[-1]:.2%}\"\n",
        "#                 with open('./checkpoint_comparisons.csv', mode='a', encoding='UTF-8') as chkpt_file:\n",
        "#                     chkpt_file.write(','.join(\n",
        "#                         [iter_count, loss_val[-1], error_val[-1]])\n",
        "#                         + '\\n'\n",
        "#                     )\n",
        "            \n",
        "#             # =========================\n",
        "#             # Update progress bar\n",
        "#             # =========================\n",
        "#             progress_bar.update(n=1)\n",
        "    \n",
        "#     return batch_num, loss_train, loss_val, error_val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WotUaj6CajG2"
      },
      "source": [
        "Original training loop (all-in-one function)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwKpe_1LCND5"
      },
      "source": [
        "üí° Better implementation of logging to a .csv file: use the `csv` module and when initializing the `csv.writer()` object, set `newline=''` to prevent an extra line in between each row.\n",
        "\n",
        "See:\n",
        "* https://www.adamsmith.haus/python/answers/how-to-write-to-a-%60.csv%60-file-without-blank-lines-in-python\n",
        "* https://docs.python.org/3/library/csv.html?highlight=csv#id3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG0QOZxo7R8Q"
      },
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    num_epochs,\n",
        "    batch_size = 4,\n",
        "    save_every = 500,\n",
        "    grad_accum_factor = 8,\n",
        "    vocab_dict = token_to_index,\n",
        "    device = device):\n",
        "    \n",
        "    # Clear unused variables from memory\n",
        "    gc.collect()\n",
        "\n",
        "    # Create lists to store model performance information.\n",
        "    batch_num = []\n",
        "    loss_train = []\n",
        "    loss_val = []\n",
        "    error_val = []\n",
        "\n",
        "    # Intialize the training and validation datasets\n",
        "    train_dataset = FPHRdataset(\n",
        "        path_to_image_folder = path_to_image_folder,\n",
        "        path_to_label_folder = path_to_label_folder,\n",
        "        max_width = 2300,\n",
        "        max_height = 1900,\n",
        "        augmentation_likelihood = 0.5,\n",
        "        train = True,\n",
        "        vocab_dict = vocab_dict\n",
        "    )\n",
        "\n",
        "    val_dataset = FPHRdataset(\n",
        "        path_to_image_folder = path_to_image_folder,\n",
        "        path_to_label_folder = path_to_label_folder,\n",
        "        max_width = 2300,\n",
        "        max_height = 1900,\n",
        "        augmentation_likelihood = 0.5,\n",
        "        train = False,\n",
        "        vocab_dict = vocab_dict\n",
        "    )\n",
        "\n",
        "    # ==============================\n",
        "    # Initialize data loaders\n",
        "    # ==============================\n",
        "    # Shuffle is set to False to save memory. See: https://medium.com/@raghadalghonaim/memory-leakage-with-pytorch-23f15203faa4\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "    # ==============================\n",
        "    # Initalize model and optimizer\n",
        "    # ==============================\n",
        "\n",
        "    # Create the model and place it on the available device (CPU or GPU)\n",
        "    model = HTRmodel(\n",
        "        max_seq_len = 3500,\n",
        "        d_model = 260,\n",
        "        num_layers = 6,\n",
        "        num_heads = 4,\n",
        "        dim_feedforward = 1024,\n",
        "        encoder_name = 'resnet18',\n",
        "        drop_enc = 0.2,\n",
        "        drop_dec = 0.2,\n",
        "        activation_dec = 'gelu',\n",
        "        label_smoothing = 0,\n",
        "        vocab_len = len(vocab_dict),\n",
        "        vocab_dict = vocab_dict\n",
        "    ).to(device)\n",
        "\n",
        "    print(\"Model loaded. Total number of parameters: \"\n",
        "        f\"{sum([layer.numel() for layer in model.parameters()]):,d}.\")\n",
        "\n",
        "    # Initialize the optimizer.\n",
        "    # Hyperparameters come from Singh et al., page 9.\n",
        "    optim = torch.optim.Adam(\n",
        "        model.parameters(), lr=0.0002, betas=(0.9, 0.999)\n",
        "    )\n",
        "\n",
        "    # Determine the total number of iterations (batches) that will be processed\n",
        "    #  during training, for setting the 'total' argument of tqdm's progress bar\n",
        "    val_iterations = 5      # len(val_loader)\n",
        "    total_iterations = (\n",
        "        (num_epochs * len(train_loader))\n",
        "        + (num_epochs * ((len(train_loader) / save_every) * val_iterations))\n",
        "    )\n",
        "\n",
        "    iter_count = 0\n",
        "    iters_per_epoch = len(train_loader)\n",
        "    progress_bar = tqdm(total=total_iterations)\n",
        "    progress_bar.set_description('Beginning model training')\n",
        "    for epoch in range(num_epochs):\n",
        "        # Process one batch of images at a time in a loop through\n",
        "        #  all images in the training dataset.\n",
        "        for imgs, lbls in train_loader:\n",
        "            # Make sure the Tensors returned by the DataLoader are placed\n",
        "            #  on the GPU, to avoid running out of CPU memory.\n",
        "            #  See: https://medium.com/@raghadalghonaim/memory-leakage-with-pytorch-23f15203faa4\n",
        "            # Note that this step is unnecessary since I add the Tensors to the\n",
        "            #  correct device in the __getitem__ function of my Dataset class.\n",
        "            # imgs = imgs.to(device)\n",
        "            # lbls = lbls.to(device)\n",
        "            # imgs = imgs.cuda(non_blocking=True)\n",
        "            # lbls = lbls.cuda(non_blocking=True)\n",
        "            \n",
        "            # Clear unused variables from memory\n",
        "            gc.collect()\n",
        "\n",
        "            iter_count += 1\n",
        "\n",
        "            # Set BatchNorm and Dropout layers to training mode\n",
        "            model.train()\n",
        "\n",
        "            # Run data through model\n",
        "            train_logits, train_loss = model.forward_teacher_forcing(imgs=imgs, targets=lbls)\n",
        "            # Training predictions are not needed, so delete variable to save memory\n",
        "            del train_logits\n",
        "\n",
        "            # =========================\n",
        "            # Update model parameters\n",
        "            # =========================\n",
        "            # See: https://stackoverflow.com/questions/62067400/understanding-accumulated-gradients-in-pytorch/62076913#62076913\n",
        "            # Scale loss by the number of batches to accumulate\n",
        "            loss = train_loss / grad_accum_factor\n",
        "            # Backpropagate loss\n",
        "            loss.backward()\n",
        "            if iter_count % grad_accum_factor == 0:\n",
        "                # Update gradients\n",
        "                optim.step()\n",
        "                # Reset gradients if accumulation factor is reached\n",
        "                optim.zero_grad()\n",
        "\n",
        "            # =========================\n",
        "            # Run validation\n",
        "            # =========================\n",
        "            if iter_count % save_every == 0:\n",
        "                with torch.no_grad():\n",
        "                    # Set model to inference mode so BatchNorm\n",
        "                    # and Dropout layers perform consistently.\n",
        "                    model.eval()\n",
        "                    # Capture all losses and errors to average at the end.\n",
        "                    val_losses = []\n",
        "                    val_errors = []\n",
        "                    # Loop through validation set.\n",
        "                    for i, (val_imgs, val_lbls) in enumerate(val_loader):\n",
        "                        # val_imgs = val_imgs.to(device)\n",
        "                        # val_lbls = val_lbls.to(device)\n",
        "                        val_logits, val_predictions, val_loss = model.forward(imgs=val_imgs, targets=val_lbls)\n",
        "                        val_error = model.calculate_char_error(predictions=val_predictions, targets=val_lbls)\n",
        "                        val_losses.append(val_loss.item())\n",
        "                        val_errors.append(float(val_error))\n",
        "                        progress_bar.set_description(\n",
        "                            f\"Validation loop ({i+1}/{val_iterations}). \"\n",
        "                            f\"Loss: {val_losses[-1]:,.2f}, \"\n",
        "                            f\"Error: {val_errors[-1]:.1%} | \")\n",
        "                        progress_bar.update(n=1)\n",
        "                        progress_bar.refresh()\n",
        "                        if i > val_iterations:\n",
        "                            break\n",
        "                # Average the loss and error values\n",
        "                val_loss = np.mean(val_losses)\n",
        "                val_error = np.mean(val_errors)\n",
        "\n",
        "                # =========================\n",
        "                # Log model metrics\n",
        "                # =========================\n",
        "                batch_num.append(iter_count)\n",
        "                loss_train.append(train_loss.detach().item())\n",
        "                loss_val.append(val_loss)\n",
        "                error_val.append(val_error)\n",
        "                with open('./model_performance.csv', mode='a', encoding='UTF-8') as csv_file:\n",
        "                    csv_file.write(','.join(\n",
        "                        list(zip(batch_num, loss_train, loss_val, error_val))[-1])\n",
        "                        + '\\n'\n",
        "                    )\n",
        "                progress_bar.set_description(\n",
        "                    f\"Resuming training. \"\n",
        "                    f\"Epoch: {epoch+1}/{num_epochs}, \"\n",
        "                    f\"Avg. val. loss: {loss_val[-1]:,.2f}, \"\n",
        "                    f\"Avg. val. error: {error_val[-1]:.1%} | \")\n",
        "\n",
        "                # =========================\n",
        "                # Save checkpoint\n",
        "                # =========================\n",
        "                checkpoint_dict = {\n",
        "                    'model_state': model.state_dict(),\n",
        "                    'optimizer_state': optim.state_dict(),\n",
        "                    'epoch_num': epoch + 1,\n",
        "                    'loss': loss_val[-1]\n",
        "                }\n",
        "                checkpoint_filepath = f\"./checkpoint_{str(iter_count).zfill(5)}.pt\"\n",
        "                torch.save(checkpoint_dict, checkpoint_filepath)\n",
        "\n",
        "                # Save model accuracy\n",
        "                # model_info = f\"Epoch: {str(epoch+1).zfill(2)}, loss: {loss_val[-1]:,.3f}, character error rate: {error_val[-1]:.2%}\"\n",
        "                with open('./checkpoint_comparisons.csv', mode='a', encoding='UTF-8') as chkpt_file:\n",
        "                    chkpt_file.write(','.join(\n",
        "                        [iter_count, loss_val[-1], error_val[-1]])\n",
        "                        + '\\n'\n",
        "                    )\n",
        "            \n",
        "            # =========================\n",
        "            # Update progress bar\n",
        "            # =========================\n",
        "            progress_bar.update(n=1)\n",
        "    \n",
        "    return batch_num, loss_train, loss_val, error_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMIUaI3EEe4m",
        "outputId": "a54ffdfe-5ed8-49d3-945d-6db022da4602"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   28160 B  |   28160 B  |   28160 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |   28160 B  |   28160 B  |   28160 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   28160 B  |   28160 B  |   28160 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |   28160 B  |   28160 B  |   28160 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    2048 KB |    2048 KB |    2048 KB |       0 B  |\n",
            "|       from large pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
            "|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2020 KB |    2020 KB |    2020 KB |       0 B  |\n",
            "|       from large pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
            "|       from small pool |    2020 KB |    2020 KB |    2020 KB |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |       1    |       1    |       1    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       1    |       1    |       1    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |       1    |       1    |       1    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       1    |       1    |       1    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |       1    |       1    |       1    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       1    |       1    |       1    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       1    |       1    |       1    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       1    |       1    |       1    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.memory_summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STRpa-1GADAo"
      },
      "outputs": [],
      "source": [
        "# Make sure to run the code in the \"Metrics and utility functions\" section to\n",
        "# initialize the vocabulary dictionaries:\n",
        "#   token_to_index is a dictionary with vocabulary as keys and indexes as values.\n",
        "#   index_to_token is the inverse: a dictionary with indexes as keys as vocab as values.\n",
        "\n",
        "# Set the number of epochs to use for training.\n",
        "# Note that a model checkpoint will be saved at the end of\n",
        "# every epoch.\n",
        "num_epochs = 1\n",
        "\n",
        "# Set frequency for logging performance metrics,\n",
        "# in number of batches processed.\n",
        "save_every = 400\n",
        "\n",
        "# Set gradient accumulation factor. 1 means \"reset gradient every batch\" (normal),\n",
        "# and any number greater than 1 means to accumulate gradients for that number of\n",
        "# batches. The paper authors used a gradient accumulation factor of 2 (see Singh et al., pg. 9)\n",
        "grad_accum_factor = 8\n",
        "\n",
        "# Train model and collect performance info\n",
        "batch_num, loss_train, loss_val, error_val = train_model(\n",
        "    num_epochs = num_epochs,\n",
        "    batch_size = 1,\n",
        "    save_every = save_every,\n",
        "    grad_accum_factor = grad_accum_factor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIUFh83gfZPV"
      },
      "source": [
        "# Bug: CPU RAM increases during training, leading to OOM error\n",
        "\n",
        "The problem occurs during the training iterations, not during the validation loop.\n",
        "\n",
        "Another thing to look into is the validation loop -- it takes _forever_! This could be because each token is passed through the model one at a time, rather than in parallel. Is there any way around that? Model inference follows that pattern, so any performance improvement would greatly help in applying the model to the actual pictures of my journal.\n",
        "\n",
        "**Next steps**\n",
        "* (Not sure if this would help) ... Create separate functions for the training and validation loops, so any variables used in those loops can be released as soon as an individual iteration completes.\n",
        "\n",
        "* Since the validation loop doesn't suffer from the problem, perhaps the problem is related to the computation graph? (val loop uses `with torch.no_grad()`).\n",
        "\n",
        "* Is the problem in the Dataset? That is, does loading the tensors from images and text create variable references that are not released?\n",
        "\n",
        "* Is there anywhere that I have a list of PyTorch tensor objects? If so, am I `.detach()`-ing the tensors before storing them?\n",
        "\n",
        "* If I explicitly `del` each variable in the training loop after I finish using it, will that solve the problem?\n",
        "\n",
        "* Are the tensors from the Dataset bound to the computation graph? (that is, `requires_grad_(requires_grad=True)`?)\n",
        "\n",
        "* Ensure that the computation graph exists solely on the GPU, and is not kept on the CPU. See this [PyTorch discussion on computation graphs and transfering Tensors](https://discuss.pytorch.org/t/link-between-require-grad-and-moving-tensors-between-cpu-and-gpu-memory/97504).\n",
        "\n",
        "* This could be related to computation graph calculation, perhaps with the Tensors coming from the datasets, or the loss or logits tensors from the forward pass. See this [PyTorch forum on not detaching tensors from the computation graph](https://discuss.pytorch.org/t/moving-from-gpu-to-cpu-not-freeing-gpuram/85313/5).\n",
        "\n",
        "* PyTorch automatically allocates around 2GB of CPU RAM to facilitate movement to GPU. See this [PyTorch forum on freeing CPU RAM after moving tensors to the GPU](https://discuss.pytorch.org/t/how-to-free-cpu-ram-after-module-to-cuda-device/20381).\n",
        "\n",
        "* When PyTorch first places a Tensor on the GPU, it allocates somewhere around 900MB of GPU memory for the CUDA kernels. See this [PyTorch forum discussion on clearing GPU memory after transferring back to CPU](https://discuss.pytorch.org/t/model-to-cpu-does-not-release-gpu-memory-allocated-by-registered-buffer/126102).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "There's a memory leak error in the training loop, before the validation loop begins. Somewhere in that code, something is accumulating and taking up increasing amounts of system RAM. The problem does not persist in the validation loop.\n",
        "\n",
        "Potential sources: the loss function might be hosted on the CPU, rather than the GPU. Check the `.to(device)` methods on any returned Tensor in preceding functions and classes.\n",
        "\n",
        "I don't remember this happening before, so check the areas I modified, including the decoder class and the overall model. Are there any places I store a variable that isn't used later? For example, am I storing any Tensors inside Python lists?\n",
        "\n",
        "I could also check whether I properly use `.detach().item()` to remove a Tensor from the computation graph and get just its value before storing it to a Python list.\n",
        "\n",
        "Also, see `/var/colab/app.log` to view the system logs, including where it saves cookies and the server where the Jupyter notebook is running (so I could potentially access it from VS Code). You can find logs under Runtime > View runtime logs.\n",
        "\n",
        "**Helpful resources**\n",
        "* [Super helpful article on PyTorch performance improvements](https://towardsdatascience.com/7-tips-for-squeezing-maximum-performance-from-pytorch-ca4a40951259), TowardsDataScience, from the creator of PyTorch Lightning\n",
        " - Big idea: create tensors on the device they will be hosted on, don't use `.to(device)`.\n",
        "* [CUDA memory allocation is around 400MB when first used, even with a small tensor](https://stackoverflow.com/questions/64068771/pytorch-what-happens-to-memory-when-moving-tensor-to-gpu), Stack Overflow answer\n",
        "* [Backpropagation on partial parameter groups, between CPU and GPU](https://discuss.pytorch.org/t/keeping-only-part-of-model-parameters-on-gpu/71308), PyTorch forums\n",
        "* [Really clear article on speeding up PyTorch](https://betterprogramming.pub/how-to-make-your-pytorch-code-run-faster-93079f3c1f7b), TowardsDataScience (member-only article)\n",
        "* [Ensuring that function calls don't take up extra memory](https://pythonspeed.com/articles/function-calls-prevent-garbage-collection/), PythonSpeed article\n",
        "* [Diagnosing Data Starvation: where CPU RAM limits GPU usage](https://www.willprice.dev/2021/03/27/debugging-pytorch-performance-bottlenecks.html), Will Price blog\n",
        "* [Using Memory Profiler](https://towardsdatascience.com/did-you-know-how-to-identify-variables-in-your-python-code-which-are-high-on-memory-consumption-787bef949dbd), Medium article\n",
        "* [Memory Leakage with PyTorch: 4 Tips](https://medium.com/@raghadalghonaim/memory-leakage-with-pytorch-23f15203faa4), Medium article\n",
        "* [Performance Tuning Guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#general-optimizations), PyTorch recipes\n",
        "* [PyTorch FAQ: CUDA out-of-memory errors](https://pytorch.org/docs/stable/notes/faq.html#my-model-reports-cuda-runtime-error-2-out-of-memory)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB0kxKGkrHHM",
        "outputId": "33947c97-bc83-470c-a2d8-4f4c2bf8ed3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: memory_profiler in /usr/local/lib/python3.7/dist-packages (0.60.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n"
          ]
        }
      ],
      "source": [
        "# Install memory profiler\n",
        "%pip install memory_profiler\n",
        "%load_ext memory_profiler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzfSAlElrQDw"
      },
      "outputs": [],
      "source": [
        "from memory_test import train_model_all_in_one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVBXpXt1vljL"
      },
      "outputs": [],
      "source": [
        "vocab_dict = token_to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490,
          "referenced_widgets": [
            "eeb98c1ef85a494e8ce91e4582d5086c",
            "5c2d9018ef0c41c5ae84e0b2ae2819d9",
            "66c4b774e97849879e62f007f6004e9d",
            "86486f04d2a840f4bf75ba15c5fb240b",
            "b15387c3894e49149da126cd28ea948a",
            "c7f62d01db4a4722940900e233e48fcd",
            "fe95dc7a5f484ceea4c42b716f4379a7",
            "2771c45dc0c14a26b9dbdff418b57baf",
            "f3ef33fd18034d7d89932de7f84417d7",
            "5b8392b0c80846159efe5f23b52cafb6",
            "c3ce494d8bed429cb79443e77c7444bc"
          ]
        },
        "id": "3UcrwidBr6vq",
        "outputId": "66db7ac6-372e-4ab0-da50-d0f5ebc0da4a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/memory_profiler.py\", line 845, in enable\n",
            "    sys.settrace(self.trace_memory_usage)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded. Total number of parameters: 17,833,280.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eeb98c1ef85a494e8ce91e4582d5086c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/8100.0 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/memory_profiler.py\", line 848, in disable\n",
            "    sys.settrace(self._original_trace_function)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%mprun -f train_model_all_in_one train_model_all_in_one(num_epochs=1, vocab_dict=token_to_index, device=device, FPHRdataset=FPHRdataset, HTRmodel=HTRmodel, batch_size = 1, save_every = 400, grad_accum_factor = 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Iyj3IBZKllg",
        "outputId": "d550c670-be34-45de-f180-45b794f8e371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clearing memory...\n",
            "Afterwards, remember to re-run this notebook from the beginning.\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  400214 KB |    8747 MB |    4737 GB |    4736 GB |\n",
            "|       from large pool |  315294 KB |    8649 MB |    4729 GB |    4728 GB |\n",
            "|       from small pool |   84919 KB |     100 MB |       7 GB |       7 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  400214 KB |    8747 MB |    4737 GB |    4736 GB |\n",
            "|       from large pool |  315294 KB |    8649 MB |    4729 GB |    4728 GB |\n",
            "|       from small pool |   84919 KB |     100 MB |       7 GB |       7 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    2022 MB |    9558 MB |    9558 MB |    7536 MB |\n",
            "|       from large pool |    1934 MB |    9452 MB |    9452 MB |    7518 MB |\n",
            "|       from small pool |      88 MB |     106 MB |     106 MB |      18 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1631 MB |    1700 MB |    1542 GB |    1540 GB |\n",
            "|       from large pool |    1626 MB |    1698 MB |    1533 GB |    1531 GB |\n",
            "|       from small pool |       5 MB |       7 MB |       8 GB |       8 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1007    |    1298    |  198901    |  197894    |\n",
            "|       from large pool |     104    |     299    |  122435    |  122331    |\n",
            "|       from small pool |     903    |    1001    |   76466    |   75563    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1007    |    1298    |  198901    |  197894    |\n",
            "|       from large pool |     104    |     299    |  122435    |  122331    |\n",
            "|       from small pool |     903    |    1001    |   76466    |   75563    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      70    |     132    |     132    |      62    |\n",
            "|       from large pool |      26    |      79    |      79    |      53    |\n",
            "|       from small pool |      44    |      53    |      53    |       9    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      48    |      89    |   93221    |   93173    |\n",
            "|       from large pool |      13    |      62    |   65744    |   65731    |\n",
            "|       from small pool |      35    |      38    |   27477    |   27442    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =======================================\n",
        "# OPTIONAL: Clear GPU memory.\n",
        "# Run this cell if the training\n",
        "#   loop encounters an error and\n",
        "#   memory is not released automatically.\n",
        "# Afterwards, you'll need to re-run the\n",
        "#   notebook starting from the cell after\n",
        "#   library imports.\n",
        "# =======================================\n",
        "CLEAR_GPU_MEMORY = False\n",
        "\n",
        "if CLEAR_GPU_MEMORY:\n",
        "    print('Clearing memory...')\n",
        "    print('Afterwards, remember to re-run this notebook from the beginning.')\n",
        "    # Remove all variables that reference PyTorch objects\n",
        "    del FPHRdataset\n",
        "    del ResNetEncoder\n",
        "    del TransformerDecoder\n",
        "    del HTRmodel\n",
        "    del PositionalEncoding1D\n",
        "    del PositionalEncoding2D\n",
        "    del train_model\n",
        "    del CharacterErrorRate\n",
        "    del text_to_tensor\n",
        "    del tensor_to_text\n",
        "    del token_to_index\n",
        "    del index_to_token\n",
        "    # Clear CUDA cache\n",
        "    torch.cuda.empty_cache()\n",
        "    # Examine GPU memory usage\n",
        "    print(torch.cuda.memory_summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3Kh-Bu_HUle"
      },
      "source": [
        "## Plots of model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlI8zkqyHXeG"
      },
      "outputs": [],
      "source": [
        "plt.title('Model loss')\n",
        "plt.plot(batch_num, loss_train, 'tan', label='Training loss')\n",
        "plt.plot(batch_num, loss_val, 'slateblue', label='Validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cy2g74MOYaXE"
      },
      "outputs": [],
      "source": [
        "plt.title('Model error during training')\n",
        "plt.plot(batch_num, error_val, 'darkred', label='Validation error %')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIov2k_dHX9U"
      },
      "source": [
        "## Inference: Testing model on journal pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wXQsPapHcMG"
      },
      "outputs": [],
      "source": [
        "# Use model.eval() to set the model in evaluation (inference)\n",
        "#   mode for the BatchNorm and Dropout modules.\n",
        "# Also use the \"with torch.no_grad():\" context manager\n",
        "#   to reduce computational load by not calculating gradient information."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Deep_Learning_Final_Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0346b1dc44354c10b641014bb21dcedb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2771c45dc0c14a26b9dbdff418b57baf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "514b204d031c4b64a5abea72e0477ec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e28b8faedc854ed79d9009ba2daa0c76",
              "IPY_MODEL_871780eaabb94bcf961cd1b36c897488",
              "IPY_MODEL_b71150e4618c439880834b3880afe0c7"
            ],
            "layout": "IPY_MODEL_0346b1dc44354c10b641014bb21dcedb"
          }
        },
        "5b8392b0c80846159efe5f23b52cafb6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c2d9018ef0c41c5ae84e0b2ae2819d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7f62d01db4a4722940900e233e48fcd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fe95dc7a5f484ceea4c42b716f4379a7",
            "value": "Beginning model training:   0%"
          }
        },
        "66c4b774e97849879e62f007f6004e9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2771c45dc0c14a26b9dbdff418b57baf",
            "max": 8100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3ef33fd18034d7d89932de7f84417d7",
            "value": 7
          }
        },
        "6abed2ad3995436aba7ac4ec32fa88e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c66a28dff494241bca0cd458d0fe31e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "767940fcc9324e26be496e90d6a5d880": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86486f04d2a840f4bf75ba15c5fb240b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b8392b0c80846159efe5f23b52cafb6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c3ce494d8bed429cb79443e77c7444bc",
            "value": " 7/8100.0 [00:08&lt;3:03:02,  1.36s/it]"
          }
        },
        "871780eaabb94bcf961cd1b36c897488": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_baa7ec24aeda46738bfbb68db0897761",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_767940fcc9324e26be496e90d6a5d880",
            "value": 10
          }
        },
        "87f1f74416e24df4bde1927114be8bd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b15387c3894e49149da126cd28ea948a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b71150e4618c439880834b3880afe0c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6abed2ad3995436aba7ac4ec32fa88e6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6c66a28dff494241bca0cd458d0fe31e",
            "value": " 0/10 [00:00&lt;?, ?it/s]"
          }
        },
        "baa7ec24aeda46738bfbb68db0897761": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3ce494d8bed429cb79443e77c7444bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7f62d01db4a4722940900e233e48fcd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e28b8faedc854ed79d9009ba2daa0c76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f215b8f5e4744000b3c11354e51312bc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_87f1f74416e24df4bde1927114be8bd0",
            "value": "Reading files in dataset:   0%"
          }
        },
        "eeb98c1ef85a494e8ce91e4582d5086c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c2d9018ef0c41c5ae84e0b2ae2819d9",
              "IPY_MODEL_66c4b774e97849879e62f007f6004e9d",
              "IPY_MODEL_86486f04d2a840f4bf75ba15c5fb240b"
            ],
            "layout": "IPY_MODEL_b15387c3894e49149da126cd28ea948a"
          }
        },
        "f215b8f5e4744000b3c11354e51312bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3ef33fd18034d7d89932de7f84417d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe95dc7a5f484ceea4c42b716f4379a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
