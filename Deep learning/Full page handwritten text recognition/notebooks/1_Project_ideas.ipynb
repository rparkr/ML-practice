{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6XBbVNzJnI0"
   },
   "source": [
    "---\n",
    "# Other ideas\n",
    "* **Vehicle models through time:** input a year and an original image and have the network return a picture that shows that model at that year (e.g., to predict future models)\n",
    "* **Movie quotes:** Input a movie quote, and the model returns the movie it is (likely) from. Could be trained on movie scripts, such as from the Stands4 network.\n",
    "* **Instrumental music:** input a music file, and the network outputs that same song, but instrumental (i.e., no lyrics). Could be trained on tons of popular songs and instrumental covers of those songs, from The Piano Guys or Taryn Harbridge).\n",
    "* **Image to music:** take an image of a music sheet (i.e., notes on a page) and the network outputs the music from those notes.\n",
    "* **Colorize grayscale video in real-time:** Using footage from our Wyze Cam, train a colorizer model to add color to grayscale videos in real-time.\n",
    "* **Tune, Voice, and Lyrics mixing:** Train a model to separate voice from melody (tune) and lyrics so that it could emulate any artist singing any lyrics to any tune. For example, have Carrie Underwood sing \"I am a Child of God.\" This is basically an algorithm to generate song covers (artists singing each other's songs).\n",
    "* **Animals in the clouds:** Use a [CycleGAN model](https://junyanz.github.io/CycleGAN/) to map from cloudscapes into animals, to \"imagine\" the clouds as animals.\n",
    "* **[NeRF model](https://paperswithcode.com/method/nerf) to create 3D views** looking inward, the opposite of a panorama. It would be amazing to deploy a model like that on my Google Pixel's camera app so I could take inward-facing panoramas.\n",
    "* **GAN that generates faces conditioned on a first name**, or that predicts a name given a face. Since people often say things like: \"Is your name Tyler? You look like a Tyler to me.\" Pictures could be scraped from profile photos or some dataset like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio file to text summary\n",
    "Given an audio file (either extracted from a video or a plain audio file), transcribe the audio and return a generated (abstractive, not extractive) summary.\n",
    "\n",
    "\n",
    "Summarize the main points from an audio file (TED talk, General Conference address, BYU speeches address, podcast). Required training data includes transcribed audio, written summary. A more involved version would also process video frames to determine context and speaker, but that could take a ton of computing power, so I think I'll limit the network to extracting the audio channel and just using that data to create text and then summarize that text.\n",
    "\n",
    "Applications:\n",
    "YouTube videos\n",
    "Podcast episodes\n",
    "News broadcasts\n",
    "Audiobooks\n",
    "Audio recordings of magazine articles\n",
    "\n",
    "In all cases, there is assumed to be a single speaker, like a lecture format or a presentation (this is less complex than a movie with multiple speakers).\n",
    "\n",
    "This would likely take two networks connected to one another: the first would transcribe audio and the second would summarize the text output by the first. It would involve word embedding, language modelling, audio processing, and two loss functions (one for the transcribed audio and one for the summarized text). The model would have to learn to output longer summaries for texts with multiple themes.\n",
    "\n",
    "\n",
    "### Other idea: IMDB movie plot summaries.\n",
    "Could take either the entire movie and return the summarized plot, or the movie title and return a summary from it. That way, you could make up movie storylines just by the title. Inputs: title, rating, year (plus hyperparameter of level of creativity or randomness). This is basically a language model trained on movie storylines. Would require word-level embeddings and a sequential like output with a vocabulary size equal to the number of unique words seen in training.\n",
    "\n",
    "### Final project proposal\n",
    "Brigham Young University CS 474 course | Winter 2022 | Dr. Dan Ventura\n",
    "\n",
    "Ryan Parker\n",
    "\n",
    "19-Feb-2022\n",
    "\n",
    "**Project idea**\n",
    "\n",
    "Return a generated text summary given an audio file such as a TED talk, General Conference address, BYU devotional or forum address, or podcast. This would give me experience working with temporal (sequential) data, processing audio, and language modeling. I believe the resulting model would be a powerful way to quickly communicate the main points given a lot of unstructured information (audio files).\n",
    "\n",
    "The model would be trained on speech-heavy audio, like a lecture, a presentation, or a narration.\n",
    "\n",
    "**Applications**\n",
    "\n",
    "After training, my network could be used for summarizing any audio or video file, including YouTube videos, podcast episodes, news broadcasts, audiobooks, or audio recordings of magazine articles.\n",
    "\n",
    "**Data source**\n",
    "\n",
    "Required training data includes transcribed audio, and written summaries of the transcription. I plan to gather data from diverse sources like TED talks, podcast episodes, and general conference addresses; all of which already have transcribed audio with summaries.\n",
    "\n",
    "**My approach**\n",
    "\n",
    "My project would likely take two networks connected to one another: the first would transcribe audio and the second would summarize the text output by the first. The training process would involve word embedding, language modeling, audio processing, and two loss functions (one for the transcribed audio and one for the summarized text). The model would also have to learn to output longer summaries for texts with multiple themes, to account for the main points in each audio file.\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "A more involved version of my project would also process video frames to determine context and speaker, for summarizing movie plot lines, for example. That would take a ton of computing power (video files can easily be many GB in size), so for my project I will limit the network to processing audio only (or perhaps extracting audio from a video file) and using that data to produce text and then summarize that text.\n",
    "\n",
    "**Related research:**\n",
    "\n",
    "I recently found a [paper from 2018](https://arxiv.org/abs/1801.10198) that covers the topic of abstract summarization (generative, rather than extractive summarization). In preparation for my project, I will continue searching for research papers that approach tasks similar to the one I have proposed.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
