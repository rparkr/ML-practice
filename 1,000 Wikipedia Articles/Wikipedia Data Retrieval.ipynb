{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBvG8pNv6rnM"
      },
      "source": [
        "# Data Retrieval Project\n",
        "Getting data from the web, for use in machine learning applications throughout the rest of the course.\n",
        "\n",
        "**Requirements**\n",
        "At least 500 records of each of the following data types:\n",
        "* Numeric\n",
        "* Categorical\n",
        "* Text\n",
        "* Images\n",
        "\n",
        "Also, you need at least one _label_ to predict."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es2Bx4T27RrU"
      },
      "source": [
        "# Chosen data: 1,000 random Wikipedia articles\n",
        "For my dataset, I will use Wikipedia's API to retrieve 1,000 random articles, along with the following data about each article page:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Page ID, Title, URL, Page Views in last 60 days, Description (local), Description (Wikidata), Alias, Label, Page Size in Bytes, Count of Available Languages, Categories, Category URLs, Page Image Name, Page Image URL, Images, Image URLs, Location (Latitude), Location (Longitude), Location (Distance to BYU in meters), Location (Name), Location (Type), Location (Country), Location (Region), Location (Globe)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "In addition, I will use BeautifulSoup to retrieve the text (i.e., complete content) from each Wikipedia article returned by the API call. Those two datasets can be combined for a complete analysis across the 1,000 articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRivSxNi4P_s"
      },
      "source": [
        "# Steps for data retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIMjwmc546ql"
      },
      "outputs": [],
      "source": [
        "# import needed packages (libraries)\n",
        "import requests                 # for making web requests (API calls)\n",
        "from bs4 import BeautifulSoup   # for parsing HTML\n",
        "import pandas as pd             # for storing and analyzing data in a tabular format\n",
        "import re                       # for cleaning text data in Wikipedia articles\n",
        "import sqlite3                  # for storing a copy of the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yknZEnca4VBB"
      },
      "source": [
        "## Build API parameters for Wikipedia, `action=query`\n",
        "\n",
        "I created [this query](https://en.wikipedia.org/wiki/Special:ApiSandbox#action=query&format=json&curtimestamp=1&prop=categories%7Ccoordinates%7Cinfo%7Cpageimages%7Cpageviews%7Cdescription%7Clanglinkscount%7Cpageterms%7Cimages&indexpageids=1&generator=random&redirects=1&formatversion=2&cllimit=10&coprop=country%7Cregion%7Cglobe%7Cname%7Ctype&codistancefrompage=Brigham%20Young%20University&inprop=url&piprop=name%7Coriginal&pilimit=10&pilicense=any&pvipmetric=pageviews&descprefersource=local&imlimit=100&grnnamespace=0&grnfilterredir=nonredirects&grnlimit=5) using Wikipedia's [API sandbox](https://en.wikipedia.org/wiki/Special:ApiSandbox#)\n",
        "\n",
        "```python\n",
        "# URL string:\n",
        "URL = 'https://en.wikipedia.org/w/api.php?action=query&format=json&curtimestamp=1&prop=categories%7Ccoordinates%7Cinfo%7Cpageimages%7Cpageviews%7Cdescription%7Clanglinkscount%7Cpageterms%7Cimages&indexpageids=1&generator=random&redirects=1&formatversion=2&cllimit=10&coprop=country%7Cregion%7Cglobe%7Cname%7Ctype&codistancefrompage=Brigham%20Young%20University&inprop=url&piprop=name%7Coriginal&pilimit=10&pilicense=any&pvipmetric=pageviews&descprefersource=local&imlimit=100&grnnamespace=0&grnfilterredir=nonredirects&grnlimit=5'\n",
        "\n",
        "# OR...\n",
        "# JSON dictionary of settings\n",
        "# (see the code in cells below, this is a long list of query parameters)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D6G5BhkE_pw"
      },
      "outputs": [],
      "source": [
        "# See example at: https://www.mediawiki.org/wiki/API:Categories#Python\n",
        "\n",
        "base_url = 'https://en.wikipedia.org/w/api.php'\n",
        "\n",
        "# See: https://meta.wikimedia.org/wiki/User-Agent_policy\n",
        "your_name = 'YOUR_NAME_HERE'\n",
        "contact_info = 'EMAIL_ADDRESS'\n",
        "required_headers = {\n",
        "    'User-Agent': f'{your_name} ({contact_info}) using Python requests library'\n",
        "}\n",
        "parameters = {\n",
        "    \"action\": \"query\",  # the type of API call (in this case, a data request)\n",
        "    \"format\": \"json\",   # return format\n",
        "    \"curtimestamp\": 1,  # return the current time stamp (UTC)\n",
        "    \"prop\": \"categories|coordinates|info|pageimages|pageviews|description|langlinkscount|pageterms|images\", # additoinal page properties to include\n",
        "    \"indexpageids\": 1,              # include a list of the returned page IDs; useful for making other API calls\n",
        "    \"continue\": \"grncontinue||\",    # parameter that fetches the next batch of results\n",
        "    \"generator\": \"random\",          # return a randomly-chosen article and a sequence of articles after it\n",
        "    \"redirects\": 1,                 # automatically resolve redirects\n",
        "    \"formatversion\": \"2\",           # return data in modern (JSON) format. Other version is \"1\" (XML-compatible)\n",
        "    \"clshow\": \"!hidden\",            # include only non-hidden categories (other options are to omit this item or \"hidden\")\n",
        "    \"cllimit\": \"10\",                # the limit of categories returned for a single page (ranges from 1-5000)\n",
        "    \"coprop\": \"country|region|globe|name|type\",         # properties of article coordinates to return\n",
        "    \"codistancefrompage\": \"Brigham Young University\",   # calculate the distance (in meters) from the returned page to BYU's coordinates\n",
        "    \"inprop\": \"url\",                # additional page information: URL of the page\n",
        "    \"piprop\": \"name|original\",      # additional page image information: the name and URL of the original image (not thumbnail)\n",
        "    \"pilimit\": \"10\",                # limit of page images to return (although results still return only 1 image). Ranges from 1-50.\n",
        "    \"pilicense\": \"any\",             # include images of any license type. Other option is \"free\"\n",
        "    \"pvipmetric\": \"pageviews\",      # page view metric is set to pageviews (the only option)\n",
        "    \"descprefersource\": \"local\",    # try to find a local (formula-derived) description of the page. If not, return the \"global\" description from Wikidata\n",
        "    \"imlimit\": \"100\",               # limit of images to return (actually, returns a list of the links to images on the page). Ranges from 1-100.\n",
        "    \"grnnamespace\": \"0\",            # 0 means \"articles\". Other namespaces include media, files (images), talks/discussions, or others.\n",
        "    \"grnfilterredir\": \"nonredirects\",   # other options: \"all\", \"redirects\". \"All\" will include a list of the pages and associated redirects to those pages.\n",
        "    \"grnlimit\": \"5\"                     # the number of random pages to return. Can range from 1-500.\n",
        "    # \"grncontinue\": \"KEY_FROM_LAST_QUERY\"\n",
        "}\n",
        "\n",
        "# api_session = requests.session()\n",
        "\n",
        "# response = api_session.get(url = base_url, params = parameters, headers = required_headers)\n",
        "# json_data = response.json()\n",
        "\n",
        "# for key, value in json_data.items():\n",
        "#     print(key, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clwPfrJDJEHn"
      },
      "source": [
        "## Construct lists to hold data from query\n",
        "Each list item represents one row of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b4GphoAJXom"
      },
      "outputs": [],
      "source": [
        "# These are the columns obtained from the API call\n",
        "# Additional data scraping will retrieve other helpful data, \n",
        "# like page text (content) and last updated date\n",
        "data_columns = [\n",
        "                'Page ID', \n",
        "                'Title', \n",
        "                'URL', \n",
        "                'Page Views, last 60 days', \n",
        "                'Description, local', \n",
        "                'Description, Wikidata', \n",
        "                'Alias', \n",
        "                'Label', \n",
        "                'Size in Bytes', \n",
        "                'Available languages count', \n",
        "                'Categories', \n",
        "                'Category URLs',  \n",
        "                'Page Image Name', \n",
        "                'Page Image URL', \n",
        "                'Images', \n",
        "                'Image URLs', \n",
        "                'Location, Latitude', \n",
        "                'Location, Longitude', \n",
        "                'Location, Distance to BYU', \n",
        "                'Location, Name', \n",
        "                'Location, Type', \n",
        "                'Location, Country', \n",
        "                'Location, Region', \n",
        "                'Location, Globe' \n",
        "                ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmU0PpLGqjeU"
      },
      "source": [
        "## Create a function to extract data from API calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbHyX5BcqTGZ"
      },
      "outputs": [],
      "source": [
        "def Query_Wikipedia_API(data_list, num_pages = 5, is_continued = False, continue_key = ''):\n",
        "    '''\n",
        "    Purpose: query Wikipedia's API and return page data to a list that can be passed to a DataFrame\n",
        "    Columns in return list: 'Page ID', 'Title', 'URL', 'Page Views, last 60 days', 'Description, local', \n",
        "                'Description, Wikidata', 'Alias', 'Label', 'Size in Bytes', 'Available languages count', \n",
        "                'Categories', 'Category URLs', 'Page Image Name', 'Page Image URL', 'Images', \n",
        "                'Image URLs', 'Location, Latitude', 'Location, Longitude', 'Location, Distance to BYU', \n",
        "                'Location, Name', 'Location, Type', 'Location, Country', 'Location, Region', 'Location, Globe' \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_list: required. List object to store the values returned by the API call. Will be modified by this function.\n",
        "    num_pages: Optional, default = 5 (range: 1-500). The number of random pages to request from Wikipedia.\n",
        "        Note that Wikipedia will not reliably return Category or Page View information if num_pages is > 5.\n",
        "    is_continued: Optional, default = False. If this is the second or later query, set to True to get a new resultset\n",
        "    continue_key: Optional, default is ''. If is_continued is True, set continue_key to the value \n",
        "        returned by the previous request\n",
        "    '''\n",
        "    import requests\n",
        "    import urllib.parse\n",
        "    from bs4 import BeautifulSoup\n",
        "\n",
        "    json_params = {\n",
        "    \"action\": \"query\",  # the type of API call (in this case, a data request)\n",
        "    \"format\": \"json\",   # return format\n",
        "    \"curtimestamp\": 1,  # return the current time stamp (UTC)\n",
        "    \"prop\": \"categories|coordinates|info|pageimages|pageviews|description|langlinkscount|pageterms|images\", # additoinal page properties to include\n",
        "    \"indexpageids\": 1,              # include a list of the returned page IDs; useful for making other API calls\n",
        "    \"continue\": \"grncontinue||\",    # parameter that fetches the next batch of results\n",
        "    \"generator\": \"random\",          # return a randomly-chosen article and a sequence of articles after it\n",
        "    \"redirects\": 1,                 # automatically resolve redirects\n",
        "    \"formatversion\": \"2\",           # return data in modern (JSON) format. Other version is \"1\" (XML-compatible)\n",
        "    \"clshow\": \"!hidden\",            # include only non-hidden categories (other options are to omit this item or \"hidden\")\n",
        "    \"cllimit\": \"10\",                # the limit of categories returned for a single page (ranges from 1-5000)\n",
        "    \"coprop\": \"country|region|globe|name|type\",         # properties of article coordinates to return\n",
        "    \"codistancefrompage\": \"Brigham Young University\",   # calculate the distance (in meters) from the returned page to BYU's coordinates\n",
        "    \"inprop\": \"url\",                # additional page information: URL of the page\n",
        "    \"piprop\": \"name|original\",      # additional page image information: the name and URL of the original image (not thumbnail)\n",
        "    \"pilimit\": \"10\",                # limit of page images to return (although results still return only 1 image). Ranges from 1-50.\n",
        "    \"pilicense\": \"any\",             # include images of any license type. Other option is \"free\"\n",
        "    \"pvipmetric\": \"pageviews\",      # page view metric is set to pageviews (the only option)\n",
        "    \"descprefersource\": \"local\",    # try to find a local (formula-derived) description of the page. If not, return the \"global\" description from Wikidata\n",
        "    \"imlimit\": \"100\",               # limit of images to return (actually, returns a list of the links to images on the page). Ranges from 1-100.\n",
        "    \"grnnamespace\": \"0\",            # 0 means \"articles\". Other namespaces include media, files (images), talks/discussions, or others.\n",
        "    \"grnfilterredir\": \"nonredirects\",   # other options: \"all\", \"redirects\". \"All\" will include a list of the pages and associated redirects to those pages.\n",
        "    \"grnlimit\": \"5\"                 # the number of random pages to return. Can range from 1-500. Pageviews won't be shown if more than 5 pages are requested.\n",
        "    # \"grncontinue\": \"KEY_FROM_LAST_QUERY\"\n",
        "    }\n",
        "\n",
        "    # set the return count from the json_params list\n",
        "    json_params['grnlimit'] = num_pages\n",
        "\n",
        "    # add the grncontinue key and its value if this request is a subsequent one after the first\n",
        "    if is_continued:\n",
        "        json_params['grncontinue'] = continue_key\n",
        "    else:\n",
        "        if 'grncontinue' in json_params.keys():\n",
        "            del json_params['grncontinue']\n",
        "    \n",
        "    # query Wikipedia's API and return a random list of 5 articles\n",
        "    base_url = 'https://en.wikipedia.org/w/api.php'\n",
        "\n",
        "    # See: https://meta.wikimedia.org/wiki/User-Agent_policy\n",
        "    required_headers = {\n",
        "        'User-Agent': 'Student: Ryan Parker (rparker8@byu.edu) using Python requests library'\n",
        "        }\n",
        "    api_session = requests.session()\n",
        "    response = api_session.get(url = base_url, params = json_params, headers = required_headers)\n",
        "    json_data = response.json()\n",
        "\n",
        "    # record the continue key\n",
        "    response_continue_key = json_data['continue']['grncontinue']\n",
        "\n",
        "    # set up the counting variables\n",
        "    numeric_count = 0\n",
        "    categorical_count = 0\n",
        "    text_count = 0\n",
        "    image_count = 0\n",
        "    \n",
        "    # Change None values to 0 so the Page_views calculate properly\n",
        "    # This function uses recursion to trace the dictionary tree and replace None with 0\n",
        "    # See: https://stackoverflow.com/a/35986190/17005348\n",
        "    def replace_none_in_dict(any_dict):\n",
        "        # what to replace None with\n",
        "        replace_value = 0\n",
        "        \n",
        "        # recursive loop\n",
        "        for k, v in any_dict.items():\n",
        "            if v is None:\n",
        "                any_dict[k] = replace_value\n",
        "            elif type(v) == dict:\n",
        "                replace_none_in_dict(v)\n",
        "            elif type(v) == list:\n",
        "                for item in v:\n",
        "                    if item is None:\n",
        "                        item = replace_value\n",
        "                    elif type(item) == dict:\n",
        "                        replace_none_in_dict(item)\n",
        "    \n",
        "    replace_none_in_dict(json_data)\n",
        "\n",
        "    # parse json_data to get information for each variable\n",
        "    for item in json_data['query']['pages']:\n",
        "        Page_ID = item.get('pageid', '')\n",
        "        \n",
        "        Title = item.get('title', '')\n",
        "        \n",
        "        URL = item.get('fullurl', '')\n",
        "        \n",
        "        try:\n",
        "            Page_views = sum([value for key, value in item['pageviews'].items()])\n",
        "            numeric_count += 1\n",
        "        except:\n",
        "            Page_views = 0\n",
        "\n",
        "        Desc_loc = item.get('description', '')\n",
        "        if Desc_loc != '':\n",
        "            text_count += 1\n",
        "\n",
        "        try:\n",
        "            Desc_global = item['terms']['description'][0]\n",
        "        except:\n",
        "            Desc_global = ''\n",
        "        \n",
        "        try:\n",
        "            # Alias = ', '.join([a for a in item['terms']['alias']])\n",
        "            Alias = [a for a in item['terms']['alias']]\n",
        "        except:\n",
        "            Alias = ''\n",
        "        \n",
        "        try:\n",
        "            # Label = ', '.join([l for l in item['terms']['label']])\n",
        "            Label = [l for l in item['terms']['label']]\n",
        "        except:\n",
        "            Label = ''\n",
        "\n",
        "        Byte_length = item.get('length', '')\n",
        "\n",
        "        Num_languages = item.get('langlinkscount', '')\n",
        "\n",
        "        try:\n",
        "            Categories = [cat['title'][9:] for cat in item['categories']]   # \"Category:\" is 9 characters, so we'll trim that off of the results\n",
        "            categorical_count += 1\n",
        "        except:\n",
        "            Categories = ''\n",
        "\n",
        "        if len(Categories) > 0:\n",
        "            try:\n",
        "                Category_URLs = ['https://en.wikipedia.org/wiki/Category:' + cat.replace(' ', '_') for cat in Categories]\n",
        "            except:\n",
        "                Category_URLs = ''\n",
        "        else:\n",
        "            Category_URLs = ''\n",
        "        \n",
        "\n",
        "        Page_Img_Name = item.get('pageimage', '')\n",
        "        \n",
        "        try:\n",
        "            Page_Img_URL = item['original']['source']\n",
        "        except:\n",
        "            Page_Img_URL = ''\n",
        "\n",
        "        try:\n",
        "            Images = ['https://en.wikipedia.org/wiki/' + file['title'].replace(' ', '_') for file in item['images']]\n",
        "            image_count += len(Images)\n",
        "        except:\n",
        "            Images = ''\n",
        "        \n",
        "        if len(Images) > 0: \n",
        "            try:\n",
        "                img_url_list = []\n",
        "                for x, list_item in enumerate(Images):\n",
        "                    image_title = item['images'][x]['title']\n",
        "                    soup = BeautifulSoup(requests.get(list_item).text, 'html')\n",
        "                    direct_url = 'https:' + soup.find(\"div\", {'class':'fullImageLink'}).find('a')['href']\n",
        "                    img_url_list.append(direct_url)\n",
        "                Image_URLs = img_url_list\n",
        "            except:\n",
        "                Image_URLs = ''\n",
        "        else:\n",
        "            Image_URLs = ''\n",
        "        \n",
        "        try: \n",
        "            Loc_Lat = item['coordinates'][0]['lat']\n",
        "        except: \n",
        "            Loc_Lat = ''\n",
        "\n",
        "        try:\n",
        "            Loc_Lon = item['coordinates'][0]['lon']\n",
        "        except:\n",
        "            Loc_Lon = ''\n",
        "        \n",
        "        try: \n",
        "            Loc_Dist = item['coordinates'][0]['dist']\n",
        "        except:\n",
        "            Loc_Dist = ''\n",
        "        \n",
        "        try:\n",
        "            Loc_Name = item['coordinates'][0]['name'] \n",
        "        except:\n",
        "            Loc_Name = ''\n",
        "\n",
        "        try:\n",
        "            Loc_Type = item['coordinates'][0]['type']\n",
        "        except:\n",
        "            Loc_Type = ''\n",
        "        \n",
        "        try: \n",
        "            Loc_Country = item['coordinates'][0]['country']\n",
        "        except:\n",
        "            Loc_Country = ''\n",
        "        \n",
        "        try:\n",
        "            Loc_Region = item['coordinates'][0]['region']\n",
        "        except: \n",
        "            Loc_Region = ''\n",
        "        \n",
        "        try:\n",
        "            Loc_Globe = item['coordinates'][0]['globe']\n",
        "        except:\n",
        "            Loc_Globe = ''\n",
        "        \n",
        "        # combine all values into one row\n",
        "        one_row = [\n",
        "                   Page_ID, \n",
        "                   Title,\n",
        "                   URL, \n",
        "                   Page_views, \n",
        "                   Desc_loc, \n",
        "                   Desc_global, \n",
        "                   Alias, \n",
        "                   Label, \n",
        "                   Byte_length,\n",
        "                   Num_languages, \n",
        "                   Categories, \n",
        "                   Category_URLs, \n",
        "                   Page_Img_Name, \n",
        "                   Page_Img_URL, \n",
        "                   Images, \n",
        "                   Image_URLs, \n",
        "                   Loc_Lat, \n",
        "                   Loc_Lon, \n",
        "                   Loc_Dist, \n",
        "                   Loc_Name, \n",
        "                   Loc_Type, \n",
        "                   Loc_Country, \n",
        "                   Loc_Region, \n",
        "                   Loc_Globe    \n",
        "        ]\n",
        "\n",
        "        # add row to data_list\n",
        "        data_list.append(one_row)\n",
        "    \n",
        "    return_dict = {\n",
        "        # 'data_list': data_list,               # Not necessary because the data_list is updated within the function\n",
        "        'continue_key': response_continue_key, \n",
        "        'page_ids': json_data['query']['pageids'], \n",
        "        'numeric_count': numeric_count, \n",
        "        'categorical_count': categorical_count, \n",
        "        'text_count': text_count, \n",
        "        'images_count': image_count\n",
        "    }\n",
        "\n",
        "    return return_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDkN-EJGqpP3"
      },
      "source": [
        "## Loop through API calls until data requirements are met"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8T-3PFRqtGr"
      },
      "outputs": [],
      "source": [
        "data_rows = []\n",
        "\n",
        "pageid_list = []        # list of all Wikipedia page IDs returned from the queries\n",
        "cont_key = ''           # unique key to get next batch of random articles\n",
        "num_count = 0\n",
        "catg_count = 0\n",
        "txt_count = 0\n",
        "img_count = 0\n",
        "data_requirements_met = False\n",
        "is_first_request = True\n",
        "\n",
        "while not data_requirements_met:\n",
        "\n",
        "    # Code below commented out for testing purposes. \n",
        "    # I believe that the 'grncontinue' parameter causes the query to be replicated, \n",
        "    # which is not what we want; rather, we want new data each time\n",
        "    # if is_first_request:\n",
        "    #     result_dict = Query_Wikipedia_API(data_list=data_rows, num_pages=5)\n",
        "    #     is_first_request = False\n",
        "    # else:\n",
        "    #     result_dict = Query_Wikipedia_API(data_list=data_rows, num_pages=5, is_continued=True, continue_key=cont_key)\n",
        "    \n",
        "    result_dict = Query_Wikipedia_API(data_list=data_rows, num_pages=5)\n",
        "    cont_key = result_dict['continue_key']\n",
        "    num_count += result_dict['numeric_count']\n",
        "    catg_count += result_dict['categorical_count']\n",
        "    txt_count += result_dict['text_count']\n",
        "    img_count += result_dict['images_count']\n",
        "    pageid_list.extend(result_dict['page_ids'])\n",
        "    \n",
        "    print('Total numeric records:\\t', num_count)\n",
        "    print('Total categ. records:\\t', catg_count)\n",
        "    print('Total text records:\\t', txt_count)\n",
        "    print('Total image count:\\t', img_count)\n",
        "    print('Number of pages retrieved:\\t', len(pageid_list))\n",
        "    print('\\n\\n')\n",
        "\n",
        "    # Check whether there are at least 500 values of each data type\n",
        "    if (num_count >= 500) and (catg_count >= 500) and (txt_count >= 500) and (img_count >= 500):\n",
        "        data_requirements_met = True\n",
        "\n",
        "# Create a DataFrame from the results\n",
        "df = pd.DataFrame(data=data_rows, columns=data_columns)\n",
        "\n",
        "# Display DataFrame\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoT7N1LZXw4y"
      },
      "outputs": [],
      "source": [
        "# Show basic info about DataFrame\n",
        "df.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E17jakBnh1jq"
      },
      "source": [
        "## Save the DataFrame to a SQLite database and to a .csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHmyzqz6VT_0"
      },
      "outputs": [],
      "source": [
        "# NOTE: I commented-out the SQL part of the code below because SQLlite cannot\n",
        "# create a database where some values are Python lists.\n",
        "# \n",
        "# For simplicity, I used lists to store values for a page (article) when there \n",
        "# were multiple values for a single page; for example, with images or categories.\n",
        "# In proper database implementation, I would create a separate table to hold\n",
        "# those multiple values, and would link it to the main table using a mapping table.\n",
        "# Since I will use pandas to manipulate this data, I will keep it condensed for now\n",
        "# and retain the Python lists.\n",
        "\n",
        "# # First, to a SQLite database (.db file)\n",
        "# # establish connection\n",
        "# conn = sqlite3.connect('Wikipedia_data.db')\n",
        "\n",
        "# # run SQL -- this will create the table, too\n",
        "# df.to_sql(name='Wikipedia_data', con=conn, if_exists='replace', index=False)\n",
        "\n",
        "# # Note: there is no need for conn.commit(), the changes are automatically saved\n",
        "# # Close the connection\n",
        "# conn.close()\n",
        "# # ------------------------------------\n",
        "# # End of SQL section\n",
        "# # ------------------------------------\n",
        "\n",
        "\n",
        "# Next, to a .csv file\n",
        "df.to_csv('Wikipedia_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90hmHuIUn0K5"
      },
      "source": [
        "# Additional info from web scraping\n",
        "Along with the information obtained from API calls, this additional information will enhance the analysis.\n",
        "\n",
        "This information can be obtained from the article page and from the Page Information page. For example, see the [Page Information for the Wikipedia article on Brigham Young Univeristy](https://en.wikipedia.org/w/index.php?title=Brigham_Young_University&action=info/)\n",
        "\n",
        "**Information to retrieve**\n",
        "* Page content (i.e., the text of a page)\n",
        "* Date of last edit\n",
        "* Date of page creation\n",
        "* Number of redirects to page\n",
        "* Total number of edits\n",
        "* Number of edits, last 30 days\n",
        "* Recent number of distinct authors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLEHWtFiZ8ev"
      },
      "source": [
        "## Create function to retrieve and clean text from Wikipedia articles\n",
        "> This function receives a Wikipedia URL as an input and returns the full text of that article, with references removed. For example, _\"Sir Isaac Newton is credited for inventing calculus**[12]**\"_ would become _\"Sir Isaac Newton is credited for inventing calculus_,\" without the **[12]** at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6n-7worLazzl"
      },
      "outputs": [],
      "source": [
        "def wikipedia_get_text(wikipedia_url):\n",
        "    import requests\n",
        "    from bs4 import BeautifulSoup\n",
        "\n",
        "    webpage = requests.get(wikipedia_url)\n",
        "    parsed_page = BeautifulSoup(webpage.text, 'html')\n",
        "    # paragraphs = parsed_page.findAll(text=True)       # find all tags that have text, like <div>, <span> or <p>\n",
        "    paragraphs = parsed_page.findAll('p')\n",
        "\n",
        "    article_text = ''\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        article_text += '\\n\\n' + paragraph.text\n",
        "\n",
        "    # remove blank spaces before and after the article text\n",
        "    article_text = article_text.strip()\n",
        "\n",
        "    # Use the re (RegEx) library to substitute any references with an empty space\n",
        "    # See: https://www.kite.com/python/answers/how-to-use-re.sub()-in-python\n",
        "    # Also: https://docs.python.org/3/library/re.html#regular-expression-syntax\n",
        "\n",
        "    # The 'r' in front of the pattern tells Python to treat this as a raw string\n",
        "    # so any Python-specific character sequences (like /n for a new line)\n",
        "    # will be treated as ordinary text.\n",
        "    article_text = re.sub(\n",
        "        pattern = r'\\[[0-9]*\\]', # or, to remove characters too: pattern = r'\\[[a-z0-9]*\\]'\n",
        "        repl = '',\n",
        "        string = article_text)\n",
        "\n",
        "    return article_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1q0VxZFe2L5"
      },
      "source": [
        "## Create a function to retrieve additional info from the 'Page information' page for any Wikipedia article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHYcKkSve_0L"
      },
      "outputs": [],
      "source": [
        "def wikipedia_get_page_info(wikipedia_info_page_url):\n",
        "    import requests\n",
        "    from bs4 import BeautifulSoup\n",
        "    # format for a Wikipedia info page URL:\n",
        "    # https://en.wikipedia.org/w/index.php?title=[PAGE_TITLE]&action=info\n",
        "\n",
        "    webpage = requests.get(wikipedia_info_page_url)\n",
        "    soup = BeautifulSoup(webpage.text, 'html')\n",
        "\n",
        "    try:\n",
        "        Last_edited_on = soup.find('tr', {'id':'mw-pageinfo-lasttime'}).findAll('td')[1].text\n",
        "        comma_position = Last_edited_on.find(',')\n",
        "        if comma_position > -1:\n",
        "            Last_edited_on = Last_edited_on[comma_position + 1:]\n",
        "    except:\n",
        "        Last_edited_on = ''\n",
        "    \n",
        "    try:\n",
        "        Page_created_on = soup.find('tr', {'id':'mw-pageinfo-firsttime'}).findAll('td')[1].text\n",
        "        comma_position = Page_created_on.find(',')\n",
        "        if comma_position > -1:\n",
        "            Page_created_on = Page_created_on[comma_position + 2:]\n",
        "    except:\n",
        "        Page_created_on = ''\n",
        "\n",
        "    # No unique ID for number of redirects, so I use BeautifulSoup's ability to get the next elements\n",
        "    # See: https://www.kite.com/python/examples/1742/beautifulsoup-find-the-next-element-after-a-tag\n",
        "    # See also: https://www.kite.com/python/examples/1730/beautifulsoup-find-the-next-sibling-of-a-tag\n",
        "    try:\n",
        "        Num_redirects = soup.find('td', text='Number of redirects to this page').next_sibling.text\n",
        "    except:\n",
        "        try:\n",
        "            Num_redirects = soup.find('tr', {'id':'mw-pageinfo-visiting-watchers'}).next_sibling.next_sibling.findAll('td')[1].text\n",
        "        except:\n",
        "            Num_redirects = 0\n",
        "    \n",
        "    try:\n",
        "        Total_edits = soup.find('tr', {'id':'mw-pageinfo-edits'}).findAll('td')[1].text\n",
        "    except:\n",
        "        Total_edits = 0\n",
        "    \n",
        "    try:\n",
        "        Edits_30_days = soup.find('tr', {'id':'mw-pageinfo-recent-edits'}).findAll('td')[1].text\n",
        "    except:\n",
        "        Edits_30_days = 0\n",
        "    \n",
        "    try:\n",
        "        Distinct_authors_30_days = soup.find('tr', {'id':'mw-pageinfo-recent-authors'}).findAll('td')[1].text\n",
        "    except:\n",
        "        Distinct_authors_30_days = 0\n",
        "    \n",
        "\n",
        "    return_dict = {\n",
        "        \"Redirects\": Num_redirects, \n",
        "        \"Created on\": Page_created_on,\n",
        "        \"Last edited\": Last_edited_on,  \n",
        "        \"Total edits\": Total_edits, \n",
        "        \"Edits, 30 days\": Edits_30_days, \n",
        "        \"Recent authors\": Distinct_authors_30_days\n",
        "    }\n",
        "\n",
        "    return return_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHrkoFsIhm6c"
      },
      "source": [
        "## Create lists of page titles and page URLs for obtaining additional information through web scraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8tXf_YsYOZ2"
      },
      "outputs": [],
      "source": [
        "# Create a list of all page titles returned from the queries\n",
        "# This also replaces spaces with underscores to prepare for use in Page Information URLs\n",
        "page_titles = [i.replace(' ', '_') for i in list(df['Title'])]\n",
        "\n",
        "# Create a list of the URLs of all pages returned from the queries\n",
        "page_urls = list(df['URL'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSqDbb7Yhty3"
      },
      "source": [
        "## Loop through page titles to get Page Information on each article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7ZlEaBshzxU"
      },
      "outputs": [],
      "source": [
        "# Set up lists for storing returned values\n",
        "Page_info_urls = []\n",
        "Page_text = []\n",
        "Redirects = []\n",
        "Date_created = []\n",
        "Date_last_edit = []\n",
        "Total_edits = []\n",
        "Recent_edits = []\n",
        "Recent_authors = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9N6uhJtdhzgE"
      },
      "outputs": [],
      "source": [
        "# Get all Page Information data\n",
        "\n",
        "for i, pgtitle in enumerate(page_titles):\n",
        "    info_url = f'https://en.wikipedia.org/w/index.php?title={pgtitle}&action=info'\n",
        "    Page_info_urls.append(info_url)\n",
        "    \n",
        "    # Run function\n",
        "    return_dict = wikipedia_get_page_info(info_url)\n",
        "\n",
        "    # Store values\n",
        "    Redirects.append(return_dict['Redirects'])\n",
        "    Date_created.append(return_dict['Created on'])\n",
        "    Date_last_edit.append(return_dict['Last edited'])\n",
        "    Total_edits.append(return_dict['Total edits'])\n",
        "    Recent_edits.append(return_dict['Edits, 30 days'])\n",
        "    Recent_authors.append(return_dict['Recent authors'])\n",
        "\n",
        "    print('Completed page:', i + 1, 'of', len(page_titles))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZbFSmgtsk1o"
      },
      "outputs": [],
      "source": [
        "# Get all Page content (text)\n",
        "\n",
        "for i, pgurl in enumerate(page_urls):\n",
        "    Page_text.append(wikipedia_get_text(pgurl))\n",
        "    \n",
        "    print('Completed page:', i + 1, 'of', len(page_urls))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx5ip_JKs7uk"
      },
      "source": [
        "## Create a dictionary to organize lists of additional info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwdOfoZ-tEc9"
      },
      "outputs": [],
      "source": [
        "info_dict = {\n",
        "    \"Page ID\": list(df['Page ID']), \n",
        "    \"Page title\": list(df['Title']), \n",
        "    \"Page URL\": list(df['URL']), \n",
        "    \"Page Info URL\": Page_info_urls, \n",
        "    \"Page text\": Page_text, \n",
        "    \"Redirects\": Redirects, \n",
        "    \"Date created\": Date_created, \n",
        "    \"Last edited date\": Date_last_edit, \n",
        "    \"Total edits\": Total_edits, \n",
        "    \"Recent edits\": Recent_edits, \n",
        "    \"Recent authors\": Recent_authors \n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-hVR_T1tFBP"
      },
      "source": [
        "## Create a DataFrame of additional info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KM0u47dxtETS"
      },
      "outputs": [],
      "source": [
        "# This DataFrame is called df2, since the first one was just df\n",
        "\n",
        "df2 = pd.DataFrame(info_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdhMxB1q2HOg"
      },
      "outputs": [],
      "source": [
        "df2.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-F1rclBtI1b"
      },
      "source": [
        "## Save DataFrame as a SQLlite database and a .csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQZAAmmxtIZa"
      },
      "outputs": [],
      "source": [
        "# First, to a SQLite database (.db file)\n",
        "# establish connection\n",
        "conn = sqlite3.connect('Wikipedia_other_info.db')\n",
        "\n",
        "# run SQL -- this will create the table, too\n",
        "df2.to_sql(name='Wikipedia_other_info', con=conn, if_exists='replace', index=False)\n",
        "\n",
        "# Note: there is no need for conn.commit(), the changes are automatically saved\n",
        "# Close the connection\n",
        "conn.close()\n",
        "\n",
        "\n",
        "# Next, to a .csv file\n",
        "df2.to_csv('Wikipedia_other_info.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2fu7gNQu4fS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRFhOU_b50dA"
      },
      "source": [
        "# Alternative method to get article text using Wikipedia's `action=parse` API\n",
        "\n",
        "This method is not recommended -- it takes much more effort to clean the text compared to using BeautifulSoup on the article's page.\n",
        "\n",
        "Also, don't use the `action=query&prop=revisions&rvprop=content` API call, because it returns the content in _wikitext_ (not plain text or HTML), which is a strange kind of formatting that is difficult to parse and clean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2JFgvogtZqV"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWiBXanouKwj"
      },
      "outputs": [],
      "source": [
        "response = requests.get('https://en.wikipedia.org/w/api.php?action=parse&page=Earth&prop=text&formatversion=2&format=json')\n",
        "json_data = response.json()\n",
        "\n",
        "json_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRx2l1cAvKyT"
      },
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(json_data['parse']['text'], 'html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8DXf_i90OGV"
      },
      "outputs": [],
      "source": [
        "soup.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY7IoaJcvi0D"
      },
      "outputs": [],
      "source": [
        "print(soup.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQVToY2qysM5"
      },
      "outputs": [],
      "source": [
        "pagetext = soup.text\n",
        "\n",
        "# remove the 'Contents' section from the page text\n",
        "start_of_contents_section = pagetext.index('\\n\\nContents\\n\\n')\n",
        "end_of_contents_section = pagetext.index('\\n\\n\\n', start_of_contents_section)\n",
        "\n",
        "pagetext_beg = pagetext[:start_of_contents_section]\n",
        "pagetext_end = pagetext[end_of_contents_section:]\n",
        "pagetext = pagetext_beg + pagetext_end\n",
        "\n",
        "# Remove the 'See also' and 'References' sections\n",
        "end_of_text = pagetext.find('\\n\\nSee also[edit]')\n",
        "if end_of_text == -1:\n",
        "    # See also section not found, use References instead\n",
        "    end_of_text = pagetext.find('\\n\\nReferences[edit]')\n",
        "\n",
        "pagetext = pagetext[:end_of_text]\n",
        "\n",
        "# The 'r' in front of the pattern tells Python to treat this as a raw string\n",
        "# so any Python-specific character sequences (like /n for a new line)\n",
        "# will be treated as ordinary text.\n",
        "pagetext = re.sub(\n",
        "    pattern = r'\\[[0-9A-Za-z]*\\]', # or, to remove characters too: pattern = r'\\[[a-z0-9]*\\]'\n",
        "    repl = '',\n",
        "    string = pagetext)\n",
        "\n",
        "# Remove extra line breaks\n",
        "pagetext = re.sub(\n",
        "    pattern = r'\\n\\n\\n', # or, to remove characters too: pattern = r'\\[[a-z0-9]*\\]'\n",
        "    repl = '\\n\\n',\n",
        "    string = pagetext)\n",
        "\n",
        "print(pagetext)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Data Collection: 1,000 Wikipedia Articles.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
